{"systems":[{"Year":2024,"PaperTitle":"Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and re\ufb02ecting on potential bene\ufb01ts of Immersive Analytics for urban data exploration","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Trajectory","ID":1,"PaperDescription":"Loads taxi trip datasets into the visualization system","Inputs":["Raw taxi trip records (origin, destination, timestamp)"],"Outputs":["Structured dataset with spatial and temporal attributes"],"ReferenceCitation":"Section 5: We use taxi data from 2010-2015... randomly sampled subsets of data containing up to a hundred thousand taxi trips each","FeedsInto":[2,3,4]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Querying","GranularBlocks":[{"GranularBlockName":"Query Prisms","ID":2,"PaperDescription":"Enables spatio-temporal querying through 3D prisms","Inputs":["User-defined query prisms (extruded 2D polygons)"],"Outputs":["Filtered dataset matching query constraints"],"ReferenceCitation":"Section 3.1: The visual query language extends to time through 3D prisms","FeedsInto":[6,8,9,10,11,12,13]}]},{"IntermediateBlockName":"Clustering","GranularBlocks":[{"GranularBlockName":"Traffic Hotspots","ID":3,"PaperDescription":"Detects areas of high taxi activity in urban spaces","Inputs":["Taxi movement data"],"Outputs":["Clustered traffic regions"],"ReferenceCitation":"Section 5: The STC point cloud presents a clear picture of the evolution of an event over time","FeedsInto":[7]}]},{"IntermediateBlockName":"Indexing","GranularBlocks":[{"GranularBlockName":"OD Pair Indexing","ID":4,"PaperDescription":"Organizes origin-destination pairs for efficient queries","Inputs":["OD trip data"],"Outputs":["Indexed OD dataset"],"ReferenceCitation":"Section 3.1: We enable both such visualization forms in our prototype","FeedsInto":[13]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"3D Scene","ID":5,"PaperDescription":"Displays an interactive 3D city scene for immersive exploration","Inputs":["Road network data","terrain data","buildings data"],"Outputs":["Rendered 3D environment"],"ReferenceCitation":"Section 3.5: By positioning a 3D widget, the user can select any position on the map, and the closest possible image is then loaded as a scene skybox","FeedsInto":[16]},{"GranularBlockName":"Space-Time Cube","ID":6,"PaperDescription":"Uses 3D space-time cube for trajectory visualization","Inputs":["Taxi trip dataset"],"Outputs":["3D STC visualization with color-coded trips"],"ReferenceCitation":"Section 3.1: Our main idea... is to extend the visualization to encompass the temporal component, through a Space-Time Cube (STC) metaphor","FeedsInto":[14,15]},{"GranularBlockName":"Overlay (Heatmap)","ID":7,"PaperDescription":"Highlights high-density taxi activity areas as an overlay","Inputs":["Clustered taxi data"],"Outputs":["Heatmap overlay on 3D map"],"ReferenceCitation":"Section 6.4.2: Our goal is to complement Immersive TaxiVis with additional functionality, such as 2D and 3D heat map visualizations","FeedsInto":[]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Line Chart","ID":8,"PaperDescription":"Displays temporal trends of taxi trip attributes","Inputs":["Aggregated trip counts over time"],"Outputs":["Line chart embedded in STC walls"],"ReferenceCitation":"Section 5.3: The embedded plots indicate how trip numbers (c) and duration (d) vary during one week.","FeedsInto":[17]},{"GranularBlockName":"Scatter Plot (STC walls)","ID":9,"PaperDescription":"Represents taxi trip distributions over time","Inputs":["Taxi trip metrics (e.g., duration, fare)"],"Outputs":["Scatter plot embedded in STC walls"],"ReferenceCitation":"Section 5.3: The embedded plots indicate how trip numbers (c) and duration (d) vary during one week.","FeedsInto":[17]},{"GranularBlockName":"Bar Chart (STC walls)","ID":10,"PaperDescription":"Displays aggregated trip counts per category","Inputs":["Taxi trip categories (e.g., trip duration bins)"],"Outputs":["Bar chart embedded in STC walls"],"ReferenceCitation":"Section 5.3: Comparative exploratory analysis between different neighborhoods using pointer brushes and the choropleth mode brushing.","FeedsInto":[17]},{"GranularBlockName":"Scatter Plot","ID":11,"PaperDescription":"Supports comparative brushing through scatter plot panels","Inputs":["User selections on STC"],"Outputs":["Coordinated scatter plot"],"ReferenceCitation":"Section 5.3: Comparative exploratory analysis between different neighborhoods using pointer brushes and the choropleth mode brushing.","FeedsInto":[17]},{"GranularBlockName":"Bar Chart","ID":12,"PaperDescription":"Supports comparative brushing through bar chart panels","Inputs":["User selections on STC"],"Outputs":["Coordinated bar chart"],"ReferenceCitation":"Section 5.3: Comparative exploratory analysis between different neighborhoods using pointer brushes and the choropleth mode brushing.","FeedsInto":[17]},{"GranularBlockName":"Flow Map","ID":13,"PaperDescription":"Displays OD flows between key locations","Inputs":["Indexed OD trip dataset"],"Outputs":["Visualized OD flow map"],"ReferenceCitation":"Section 5: To avoid clutter, showing connecting lines between pairs is more convenient when associated with specific origin-destination filters","FeedsInto":[]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Spatiotemporal Selection","ID":14,"PaperDescription":"Allows users to create spatio-temporal queries in 3D space","Inputs":["User interaction (click, drag, scale)"],"Outputs":["Filtered dataset based on spatial-temporal constraints"],"ReferenceCitation":"Section 3.1: Instead of 2D query polygons, we support query prisms (i.e., extruded polygons) in 3D space, whose height corresponds to their temporal span","FeedsInto":[2]},{"GranularBlockName":"Temporal Selection","ID":15,"PaperDescription":"Allows users to filter data based on time intervals","Inputs":["User input (time selection)"],"Outputs":["Filtered dataset & visual update"],"ReferenceCitation":"Section 5.1: Some features, such as the ability to manually adjust point sizes, allow the user to make patterns more evident","FeedsInto":[2]},{"GranularBlockName":"Brushing","ID":17,"PaperDescription":"Enables comparative brushing for interactive filtering","Inputs":["User selection on bar charts and scatter plots"],"Outputs":["Coordinated updates in visualizations"],"ReferenceCitation":"Section 5.3: Comparative exploratory analysis between different neighborhoods using pointer brushes and the choropleth mode brushing.","FeedsInto":[11,12]}]},{"IntermediateBlockName":"Navigation","GranularBlocks":[{"GranularBlockName":"VR Navigation","ID":16,"PaperDescription":"Supports immersive movement within the 3D visualization","Inputs":["User VR controls (joystick, gaze)"],"Outputs":["Updated viewpoint in 3D space"],"ReferenceCitation":"Section 5: Users can enter the query editing mode, and activate several features we propose, such as displaying links, displaying cutting planes, displaying point projections, and changing perspectives","FeedsInto":[5]}]}],"HighBlockName":"Interaction"}]},{"Year":2018,"PaperTitle":"VAUD: A Visual Analysis Approach for Exploring Spatio-Temporal Urban Data","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Street Network","ID":1,"PaperDescription":"Loads a road network of the city from OpenStreetMap containing 34,997 nodes and 3,794 segments with a total length of 4,524 km.","Inputs":["OpenStreetMap road network data"],"Outputs":["Structured road network data with nodes and segments"],"ReferenceCitation":"Section 3.1 Data Description: 'Geographical data: A road network of the city from OpenStreetMap [19] containing 34,997 nodes and 3,794 segments with a total length of 4,524 km.'","FeedsInto":[15]},{"GranularBlockName":"POI","ID":2,"PaperDescription":"Loads POI data containing 938,712 POI locations with longitude, latitude, name, and functionality.","Inputs":["POI dataset"],"Outputs":["Structured POI dataset with attributes (longitude, latitude, name, functionality)"],"ReferenceCitation":"Section 3.1 Data Description: 'Points of Interests (POIs) data: The information of 938,712 POI locations where each record contains the longitude, latitude, name, and functionality of a structure in the urban environment.'","FeedsInto":[27,15]},{"GranularBlockName":"Street View Images","ID":3,"PaperDescription":"Loads street view data from Baidu Map service providing linked imagery of locations.","Inputs":["Street view images from Baidu Map service"],"Outputs":["Linked street view imagery for locations"],"ReferenceCitation":"Section 3.1 Data Description: 'Street view data has been downloaded from the Baidu map service to provide linked imagery of locations.'","FeedsInto":[15]},{"GranularBlockName":"Real Estate","ID":4,"PaperDescription":"Loads real estate data containing 5,684 estate records including the name, longitude, latitude, sales price, and construction year.","Inputs":["Real estate dataset"],"Outputs":["Structured real estate data with location and attributes"],"ReferenceCitation":"Section 3.1 Data Description: 'Real estate data: 5,684 estate records in residential sub-districts where each record contains the name, longitude, latitude, sales price, and the year that the building was constructed.'","FeedsInto":[19]},{"GranularBlockName":"Mobile Phone Location","ID":5,"PaperDescription":"Loads mobile phone location data with 308 billion location records of 7 million anonymized users, including User ID, cell tower ID, and timestamp.","Inputs":["Mobile phone location dataset"],"Outputs":["Spatiotemporal location data with user IDs and timestamps"],"ReferenceCitation":"Section 3.1 Data Description: 'Mobile phone location data: 308 billion location records of 7 million anonymized mobile phone users (around 50 percent of the population in the city) where each record contains an anonymous User ID, a cell tower ID, and a time stamp.'","FeedsInto":[10,16]},{"GranularBlockName":"Social Network","ID":6,"PaperDescription":"Loads 27 million mobile phone call records among 7 million users to extract social network data.","Inputs":["Mobile phone call records dataset"],"Outputs":["Extracted social network graph from call records"],"ReferenceCitation":"Section 3.1 Data Description: 'Social network data: 27 million mobile phone call records among 7 million users with each record containing two anonymous user IDs and a time stamp. A social network is also extracted from the call records.'","FeedsInto":[20]},{"GranularBlockName":"Microblog","ID":7,"PaperDescription":"Loads microblog data with 93,491 posts including textual information, timestamps, and geotags when available.","Inputs":["Microblog dataset"],"Outputs":["Structured microblog data with geotags and timestamps"],"ReferenceCitation":"Section 3.1 Data Description: 'Microblog data: 93,491 posts of a popular microblog website whose geotags fall inside the city boundaries. Each record contains textual information, a time stamp, and a geotag (when available).'","FeedsInto":[17,16]},{"GranularBlockName":"Trajectory","ID":8,"PaperDescription":"Loads 272,470,343 trajectory records for 3,691 taxis recorded every 20 seconds, including taxi ID, GPS location, speed, occupancy status, and timestamp.","Inputs":["Taxi GPS trajectory dataset"],"Outputs":["Spatiotemporal taxi trajectory data"],"ReferenceCitation":"Section 3.1 Data Description: 'Taxi GPS trajectory data: 272,470,343 trajectory records for 3,691 taxis recorded every 20 seconds where each record contains a taxi ID, a GPS location, the speed, the occupancy status, and a time stamp.'","FeedsInto":[10,28,16]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Representation","GranularBlocks":[{"GranularBlockName":"Object-Based Representation","ID":9,"PaperDescription":"Normalizes data objects into four attributes (which, where, when, what) to support cross-domain data reasoning and analysis.","Inputs":["Raw heterogeneous urban data"],"Outputs":["Unified object-based dataset with normalized attributes"],"ReferenceCitation":"Section 3.2 Data Representation: 'In our proposed framework, the geographical and time-oriented properties of objects should be normalized into a canonical space so that objects can be related by shared locations and time.'","FeedsInto":[11]},{"GranularBlockName":"Space-Time Cube","ID":10,"PaperDescription":"Constructs a canonical 3D space-time cube for storing spatiotemporal objects indexed by space and time.","Inputs":["Spatiotemporal datasets"],"Outputs":["STC 3D grid for spatiotemporal analysis"],"ReferenceCitation":"Section 3.2 Data Representation: 'We leverage the space-time-cube [22] as the canonical space for accommodating spatio-temporal objects.'","FeedsInto":[11]}]},{"IntermediateBlockName":"Querying","GranularBlocks":[{"GranularBlockName":"Atomic Query","ID":11,"PaperDescription":"Performs atomic queries on a dataset using a combination of conditions (which, when, where, what).","Inputs":["User-defined query conditions"],"Outputs":["Query results matching atomic conditions"],"ReferenceCitation":"Section 3.3 The VAUD Query Model: 'An atomic query is composed of three components: a query condition, a query operation, and query results.'","FeedsInto":[12,13]},{"GranularBlockName":"Query Combination","ID":12,"PaperDescription":"Combines atomic queries using Boolean operators (Union, Intersection, Complement) to build complex queries.","Inputs":["Multiple atomic query operations"],"Outputs":["Combined query results"],"ReferenceCitation":"Section 3.3 The VAUD Query Model: 'A combination can be a union, an intersection, or a complement.'","FeedsInto":[13]},{"GranularBlockName":"Query Sequence","ID":13,"PaperDescription":"Assembles multiple atomic queries and extractions into a query sequence for multi-source querying and reasoning.","Inputs":["Atomic queries and extraction operations"],"Outputs":["Multi-source query results"],"ReferenceCitation":"Section 3.3 The VAUD Query Model: 'By assembling an atomic query and extraction, comprehensive query operations can be executed to perform complicated tasks.'","FeedsInto":[14]}]},{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Attribute Extraction","ID":14,"PaperDescription":"Extracts specific attributes from query results for use as conditions in subsequent queries.","Inputs":["Query result objects"],"Outputs":["Extracted attribute sets (which, where, when, what)"],"ReferenceCitation":"Section 3.3 The VAUD Query Model: 'An extraction is composed of three components, the query results, an extraction operation, and a component of the object.'","FeedsInto":[15,17,18,19,20]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Map 2D","ID":15,"PaperDescription":"Displays the geographic spatial distribution of urban data such as road networks and POIs.","Inputs":["Geospatial data (road network, POIs, etc.)"],"Outputs":["2D map view"],"ReferenceCitation":"Section 4.2 The Scene View: 'In the scene view, the road network data is displayed as a list of geometric line primitives.'","FeedsInto":[21,23]},{"GranularBlockName":"Overlay (Heatmap)","ID":16,"PaperDescription":"Overlays heatmaps on the map to visualize spatial distributions of taxis, mobile phone users, and microblog posts.","Inputs":["Spatiotemporal object data"],"Outputs":["Heatmap overlays"],"ReferenceCitation":"Table 1 Visual Forms Supported in Our Interface: 'Heatmap: Geographic distribution of taxis; Geographic distribution of users; Geographic distribution of posts.'","FeedsInto":[15,21]},{"GranularBlockName":"Overlay (Glyph)","ID":27,"PaperDescription":"Displays glyphs to represent Points of Interest (POIs), such as bookstores, schools, and shopping malls.","Inputs":["POI dataset with location and category"],"Outputs":["Glyph representations of POIs on the map"],"ReferenceCitation":"Section 4.2 The Scene View: 'A point of interest, such as a bookstore, school or a shopping mall, is shown with a representative glyph.'","FeedsInto":[15,21]},{"GranularBlockName":"Overlay (Trajectories)","ID":28,"PaperDescription":"Visualizes movement trajectories of taxis and mobile phone users as polylines on the map.","Inputs":["Trajectory datasets (taxi GPS, mobile phone location)"],"Outputs":["Polyline trajectories on the 2D map"],"ReferenceCitation":"Section 4.2 The Scene View: 'The time-varying location information, such as a GPS or mobile phone trajectory, is encoded with polylines.'","FeedsInto":[15,22,21]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Line Chart","ID":17,"PaperDescription":"Displays time-series data such as microblog post counts or taxi speed trends over time.","Inputs":["Time-series data"],"Outputs":["Line chart visualization"],"ReferenceCitation":"Table 1 Visual Forms Supported in Our Interface: 'Line chart: Number of posts over time.'","FeedsInto":[25]},{"GranularBlockName":"Bar Chart","ID":18,"PaperDescription":"Displays statistical summaries of data, such as POI categories or taxi ride occupancy durations.","Inputs":["Categorical data summaries"],"Outputs":["Bar chart visualization"],"ReferenceCitation":"Table 1 Visual Forms Supported in Our Interface: 'Bar chart: The statistics of POIs over type; time duration of riding.'","FeedsInto":[25]},{"GranularBlockName":"Scatter Plot","ID":19,"PaperDescription":"Visualizes the relationship between real estate prices and counts using scatter plots.","Inputs":["Real estate data (price, count)"],"Outputs":["Scatter plot visualization"],"ReferenceCitation":"Table 1 Visual Forms Supported in Our Interface: 'Scatter plot: Price and count of real estate.'","FeedsInto":[25]},{"GranularBlockName":"Graph","ID":20,"PaperDescription":"Visualizes the social network graph extracted from mobile phone call records as a force-directed graph.","Inputs":["Social network graph data"],"Outputs":["Force-directed graph visualization"],"ReferenceCitation":"Table 1 Visual Forms Supported in Our Interface: 'Graph: force directed graph of calling network.'","FeedsInto":[25]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Area Selection","ID":21,"PaperDescription":"Enables users to select a spatial region on the map to define a query condition (where).","Inputs":["User-drawn spatial region"],"Outputs":["Spatial query condition"],"ReferenceCitation":"Section 4.3 User Interactions: 'The geographical region selection can be used to define a rectangular region of the map to be the \u201cwhere\u201d condition.'","FeedsInto":[26]},{"GranularBlockName":"Temporal Selection","ID":22,"PaperDescription":"Provides a time-picker or time wheel for selecting a time interval as a query condition (when).","Inputs":["User-selected time range"],"Outputs":["Temporal query condition"],"ReferenceCitation":"Section 4.3 User Interactions: 'A time-picker selection can be used to specify the \u201cwhen\u201d type condition.'","FeedsInto":[26]},{"GranularBlockName":"Object Selection (Scene View)","ID":23,"PaperDescription":"Allows users to select objects in the scene view for exploration or use as query inputs.","Inputs":["User selection from scene view"],"Outputs":["Selected object details"],"ReferenceCitation":"Section 4.2 The Scene View: 'When an object is selected from the result nodes, its detailed information is shown in the corresponding visualization scheme on top of the scene view.'","FeedsInto":[26]},{"GranularBlockName":"Drag-and-Drop","ID":26,"PaperDescription":"Enables the construction of visual queries using drag-and-drop interactions in the query view.","Inputs":["User drag-and-drop actions"],"Outputs":["Visual query graph"],"ReferenceCitation":"Section 4.1 The Query View: 'The query view (Fig. 4b) uses a flow metaphor to support the construction of cross-domain query tasks by means of drag-and-drop interactions.'","FeedsInto":[11]}]},{"IntermediateBlockName":"Comparison","GranularBlocks":[{"GranularBlockName":"Compare Panel","ID":25,"PaperDescription":"Supports side-by-side comparison of selected statistical charts or metrics for deeper analysis.","Inputs":["Selected statistical graphs from scene view"],"Outputs":["Comparison view of graphs"],"ReferenceCitation":"Section 4.2 The Scene View: 'The comparison panel supports the comparison of selected statistical graphs from the scene view.'","FeedsInto":[]}]}],"HighBlockName":"Interaction"}]},{"Year":2024,"PaperTitle":"MetroBUX: A Topology-Based Visual Analytics for Bus Operational Uncertainty EXploration","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Bus Stops","ID":1,"PaperDescription":"Loads the bus stop metadata (each stop has ID, lat, lon).","Inputs":["CSV / database of stops with lat/lon."],"Outputs":["In-memory data structure of bus stops."],"ReferenceCitation":"Sec. IV-A: Stop dataset includes the information of all bus stops","FeedsInto":[4,7,14]},{"GranularBlockName":"Bus Routes","ID":2,"PaperDescription":"Loads the bus route data that describes the sequence of stops, route ID, and direction.","Inputs":["Route definitions (e.g., text or DB) specifying the order of stops."],"Outputs":["Structured route objects capturing each routes stop sequence."],"ReferenceCitation":"Sec. IV-A: Route dataset includes the information of bus routes","FeedsInto":[4,10,13]},{"GranularBlockName":"Bus Operational Dataset","ID":3,"PaperDescription":"Loads daily bus operational logs: arrival times at each stop for each bus ID and trip.","Inputs":["Raw operational logs with timestamps, stop IDs, route IDs, directions."],"Outputs":["In-memory record of arrival times (one log entry per route, stop, time)."],"ReferenceCitation":"Sec. IV-A: Operational data include the daily bus operation data that record the times a bus arrives","FeedsInto":[4]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Preprocessing","GranularBlocks":[{"GranularBlockName":"Operational Data","ID":4,"PaperDescription":"Removes duplicated operational records and fills in missing arrival times if the bus did not physically stop.","Inputs":["Raw operational logs with possible duplicates or null fields."],"Outputs":["Cleaned dataset with consistent arrival times."],"ReferenceCitation":"Sec. IV-B: We remove duplicates fill in the missing values before the followup analyses.","FeedsInto":[5]}]},{"IntermediateBlockName":"Ordering","GranularBlocks":[{"GranularBlockName":"Trip Arrivals","ID":5,"PaperDescription":"Aligns bus trips across multiple days, assigning trip orders so that arrival times can be compared day-to-day.","Inputs":["Cleaned operational logs, with partial day-level trip groupings."],"Outputs":["Aligned multi-day bus trips (each trip has consistent trip order)."],"ReferenceCitation":"Sec. IV-C.1: After connecting bus trips we further align bus trips spanning different days to model bus operation uncertainty","FeedsInto":[6,12]}]},{"IntermediateBlockName":"Uncertainty Calculation","GranularBlocks":[{"GranularBlockName":"Trip Arrivals","ID":6,"PaperDescription":"Derives the bus arrival time uncertainty (standard deviation) at each stop, treating arrival times as normally distributed.","Inputs":["Aligned arrival times from multiple days."],"Outputs":["Numeric measure (x) capturing stop-level uncertainty for each trip."],"ReferenceCitation":"Sec. IV-C.2: We set uncertainty of bus arrival times as xs,o","FeedsInto":[7,12,15,16]}]},{"IntermediateBlockName":"Kernel Density Estimation","GranularBlocks":[{"GranularBlockName":"Uncertainty Trip Values","ID":7,"PaperDescription":"Uses Kernel Density Estimation on stop-level uncertainties to produce a continuous uncertainty density map over the city.","Inputs":["Stop-level uncertainty values","geospatial stop locations."],"Outputs":["2D continuous field visualizing overall region-level uncertainty distribution."],"ReferenceCitation":"Sec. V-A: We employ kernel density estimation for regional uncertainty modeling and representation","FeedsInto":[8,9]}]},{"IntermediateBlockName":"Representation","GranularBlocks":[{"GranularBlockName":"KDE Contours","ID":8,"PaperDescription":"Builds a multi-level topological representation of how high/medium/low uncertainty regions evolve over time (split/merge).","Inputs":["KDE-based uncertainty contours at multiple thresholds over a time series."],"Outputs":["Nested tracking graph capturing region changes (merging, splitting) for each uncertainty level."],"ReferenceCitation":"Sec. V-B: We further employ a nested tracking graph to represent the evolution of uncertainty over time","FeedsInto":[11]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Map 2D","ID":9,"PaperDescription":"Displays the spatio-temporal uncertainty density using color-coded maps and contour lines of multiple uncertainty levels.","Inputs":["2D map layer"],"Outputs":["Interactive 2D map"],"ReferenceCitation":"Sec. VI, Map View allows users to explore the spatial distribution of uncertainty (Fig. 7 (d))","FeedsInto":[17,19]},{"GranularBlockName":"Overlay (Routes)","ID":10,"PaperDescription":"Overlays additional context (e.g., major roads, route lines, city boundaries) or emphasizes selected subregions.","Inputs":["Base map","route geometry","user selection"],"Outputs":["Color-coded or highlighted overlays that enhance the maps context (e.g., bus routes)."],"ReferenceCitation":"Sec. VI (and possibly VI-F): Overlays can highlight selected area show route geometry","FeedsInto":[9]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Graph","ID":11,"PaperDescription":"Shows the time-based evolution of multi-level uncertainty regions (splits, merges) as a nested graph.","Inputs":["Thresholded isocontours across time."],"Outputs":["An interactive timeline graph of region-level changes, with nesting to indicate high/medium/low."],"ReferenceCitation":"Sec. VI-B: Temporal View incorporates the nested tracking graph to show the uncertainty evolution with time","FeedsInto":[18]},{"GranularBlockName":"Marey's Graph","ID":12,"PaperDescription":"Depicts trip-level uncertainty with a Marey-style diagram: the x-axis is time, y-axis is stop order, with color/size encoding uncertainty.","Inputs":["Aligned trip data","stop-level uncertainties"],"Outputs":["Graphical schedule of bus trips","visually encoding each stops uncertainty"],"ReferenceCitation":"Sec. VI-D: Trip View visualizes the trip-level uncertainty a frequency histogram plus a graphical schedule","FeedsInto":[18]},{"GranularBlockName":"Table","ID":13,"PaperDescription":"Provides a tabular list of bus routes with ID, direction, frequency histogram, headway info. May embed a bar chart or sparkline showing route statistics.","Inputs":["Route dataset"],"Outputs":["A route list table (with embedded mini-charts), letting users pick routes for further analysis."],"ReferenceCitation":"Sec. VI-A: Route List view essentially a table with route information","FeedsInto":[18]},{"GranularBlockName":"Table","ID":14,"PaperDescription":"Shows the selected stops name, routes crossing it, route-level stats or arrival-time uncertainties. Also can display arrival time distribution as small charts.","Inputs":["User selection of a stop"],"Outputs":["A table-based info panel for that stops bus routes, uncertainties, or distribution charts"],"ReferenceCitation":"Sec. VI-E: Stop View displays the relevant route IDs, arrival times for the chosen stop","FeedsInto":[18]},{"GranularBlockName":"Bar Chart","ID":15,"PaperDescription":"Depicts discrete or binned metrics (e.g., route frequencies, day-part counts). Often used for frequency histograms or grouped bar comparisons.","Inputs":["Numeric distributions","Aggregated metrics"],"Outputs":["A bar-based visualization indicating frequency","counts","other numeric categories"],"ReferenceCitation":"Sec. VI-A:  frequency histogram plus a graphical schedule","FeedsInto":[18]},{"GranularBlockName":"Line Chart","ID":16,"PaperDescription":"Presents time-series data (e.g., how average uncertainty changes across hours).","Inputs":["Time-based numeric data series."],"Outputs":["A 2D chart with lines that trend over time, can handle multiple lines for different routes or stops."],"ReferenceCitation":"Possibly implied in Sec. VI for time-based plots of headways or average uncertainty.","FeedsInto":[18]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Animation","GranularBlocks":[{"GranularBlockName":"Time Slider","ID":17,"PaperDescription":"Allows stepping or playing through time to see how uncertainty density evolves in the map.","Inputs":["Time steps (e.g. 00:00 - 23:59)."],"Outputs":["An animated sequence of the Map Views KDE-based uncertainty layers."],"ReferenceCitation":"Sec. VI-F: MetroBUX provides a quick overview of spatio-temporal changes Users can click the play button to start the animation","FeedsInto":[9,11]}]},{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Route/Trip/Stop","ID":18,"PaperDescription":"Selecting a route from Route List or a trip in Trip View, or a stop in the Map or Trip View, to highlight in all views.","Inputs":["User pointer/click event"],"Outputs":["Highlighted route/trip/stop in the interface","updated details in relevant views"],"ReferenceCitation":"Sec. VI-F: Users can select a specific time period, route, trip, or stop of interest in different views","FeedsInto":[9,10,12,13,14,15,16]},{"GranularBlockName":"Area Selection","ID":19,"PaperDescription":"Allows users to draw a free-form region in the Map View, filtering only the contained stops/routes for further inspection.","Inputs":["User-defined polygon drawn on the map"],"Outputs":["Subset of stops with color-coded average uncertainty","Route List updated for those stops"],"ReferenceCitation":"Sec. VII, We first delineate a customized region with the lasso tool the stops within that region show large variations","FeedsInto":[9,13,14]}]}],"HighBlockName":"Interaction"}]},{"Year":2019,"PaperTitle":"Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows over Time","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"3D Geometries","ID":1,"PaperDescription":"Loads and stores city/building meshes (e.g., from OpenStreetMap) for shadow computation","Inputs":["Urban building footprints","Extruded meshes"],"Outputs":["In-memory scene geometry of all buildings"],"ReferenceCitation":"Sec. 8: We use Manhattan The mesh is composed of 1.5 million triangles.","FeedsInto":[2,3,6]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Ray Tracing","GranularBlocks":[{"GranularBlockName":"Shadow Maps","ID":2,"PaperDescription":"Extends standard shadow mapping to store multiple time-step depths in one 3D texture for linear sun movement","Inputs":["City geometry","Short time interval","Sun directions for t1 and t2"],"Outputs":["A single pass that outputs a 3D shadow map capturing all minute-slices"],"ReferenceCitation":"Sec. 4: Shadow accrual maps we implicitly track the movement of the shadow itself store the depth values for the entire time range in one pass.","FeedsInto":[5,6,7,8]},{"GranularBlockName":"Inverse Shadow Maps","ID":3,"PaperDescription":"Uses a multi-level ray tracing approach from the ground plane to record each source of shadow over time","Inputs":["Ground-plane pixel set","Building geometry","Up to l=3 intersections"],"Outputs":["Bit vectors / lines describing gross or continuous shadow for each point"],"ReferenceCitation":"Sec. 5: Inverse accrual maps map each ground point p1 to the point p2 by tracing a ray in reverse light direction capturing multiple sources of shadow.","FeedsInto":[5,6,7,8]}]},{"IntermediateBlockName":"Clustering","GranularBlocks":[{"GranularBlockName":"Direction Graph","ID":4,"PaperDescription":"Clusters sun directions to reuse shadow computations for large time intervals (multi-day or multi-month)","Inputs":["All possible minute-based sun directions for a time range"],"Outputs":["A weighted graph of edges representing distinct direction bins"],"ReferenceCitation":"Sec. 6: We build a direction graph each node is a bin to significantly reduce the number of accrual maps computed.","FeedsInto":[2,3]}]},{"IntermediateBlockName":"Aggregation","GranularBlocks":[{"GranularBlockName":"Weighted Shadow Score","ID":5,"PaperDescription":"Assigns positive or negative weights per time period (e.g., winter vs. summer) to yield an overall shadow score","Inputs":["User-chosen weighting scheme","Shadow accumulation data"],"Outputs":["A single numeric measure of net shade vs. shadow effect"],"ReferenceCitation":"Sec. 7.2: We define the shadow score the user partitions time range T each portion with weight -1 <= w <= 1.","FeedsInto":[6,7,8]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"3D Scene","ID":6,"PaperDescription":"Renders a 3D map widget showing buildings and color-coded shadow accumulation","Inputs":["Camera viewpoint","Accrued shadow data","Building meshes"],"Outputs":["An interactive 3D scene with color-coded shading or shading difference"],"ReferenceCitation":"Sec. 7.1: A 3D map widget that provides spatial context user interactions cause on-the-fly shadow updates.","FeedsInto":[10,11,12]},{"GranularBlockName":"Overlay (Heatmap)","ID":7,"PaperDescription":"Depicts differences in shadow (positive or negative) for new scenarios vs. baseline","Inputs":["Baseline and new scenario shadow accumulations"],"Outputs":["A color scale with red for more shadow, blue for less shadow"],"ReferenceCitation":"Sec. 7.1: We allow the user to select a building to replace difference is visualized using a divergent color map (Fig. 8a).","FeedsInto":[6,9,10]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Line Chart","ID":8,"PaperDescription":"Shows monthly or hourly distributions of shadow area or shadow score","Inputs":["Aggregated shadow stats (area, score) across months/hours"],"Outputs":["A bar or line chart of how shadow measures vary over time"],"ReferenceCitation":"Sec. 9.2: We also show monthly distribution plots a region's shadow area is smaller in summer than winter.","FeedsInto":[9]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Temporal Selection","ID":9,"PaperDescription":"Allows user to pick the start date, number of days, and hours per day to accumulate shadows","Inputs":["UI fields for day/time intervals"],"Outputs":["Updates the shadow accumulation and re-renders the 3D map"],"ReferenceCitation":"Sec. 7.1: A time period is selected by specifying Dstart, start time tstart, number of days n, hours per day k\u2026","FeedsInto":[2,3]},{"GranularBlockName":"Area Selection","ID":10,"PaperDescription":"Enables brushing a polygon on the 3D map to restrict shadow analysis to that region","Inputs":["User-drawn polygon region in the map widget"],"Outputs":["Shadows displayed or analyzed only within the user-selected polygon"],"ReferenceCitation":"Sec. 7.1: We also allow users to brush and select polygonal regions of interest to inspect shadows\u2026","FeedsInto":[2,3]}]},{"IntermediateBlockName":"Navigation","GranularBlocks":[{"GranularBlockName":"Viewport Change","ID":11,"PaperDescription":"Panning or zooming updates the viewport, with new level-of-detail re-computation for shadows","Inputs":["Camera transforms from user's mouse/keyboard input"],"Outputs":["Recomputed partial shadow data for the new viewpoint, displayed in the 3D scene"],"ReferenceCitation":"Sec. 7.1: User interactions (pan, zoom) recompute shadows on the fly for the region in the viewport enabling LoD rendering.","FeedsInto":[2,3,6]}]},{"IntermediateBlockName":"Manipulation","GranularBlocks":[{"GranularBlockName":"3D Geometry Replacement","ID":12,"PaperDescription":"User can choose an empty lot or demolish an existing building and place a new building design","Inputs":["Geometry of new building","Chosen location to insert"],"Outputs":["Updated city mesh plus a difference shadow map showing new building's impact"],"ReferenceCitation":"Sec. 7.1: We allow the user to select either an empty building lot or an existing building difference is visualized using a divergent color map\u2026","FeedsInto":[2,3]}]}],"HighBlockName":"Interaction"}]},{"Year":2024,"PaperTitle":"TCEVis: Visual analytics of traffic congestion influencing factors based on explainable machine learning","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Street Network","ID":1,"PaperDescription":"Loads road network data from OpenStreetMap","Inputs":["Raw road network dataset"],"Outputs":["Structured road segment data"],"ReferenceCitation":"Section 3.2: Road network data consist of a dataset extracted from OpenStreetMap that provides information about a city's road network.","FeedsInto":[5,6,16]},{"GranularBlockName":"Trajectory","ID":2,"PaperDescription":"Loads taxi trajectory data for congestion analysis","Inputs":["Raw taxi trajectory data"],"Outputs":["Processed vehicle movement data"],"ReferenceCitation":"Section 3.2: Taxi trajectory data is a collection of information on the recorded movements of taxis in a city during the month of October 2021.","FeedsInto":[5,8]},{"GranularBlockName":"Air Quality","ID":3,"PaperDescription":"Loads air quality data from environmental monitoring stations","Inputs":["Air pollution measurements"],"Outputs":["Processed air quality dataset"],"ReferenceCitation":"Section 3.2: Air quality data refers to information gathered from Chinas General Environmental Monitoring Station. This data contains measurements of different air pollutants taken in real-time.","FeedsInto":[7]},{"GranularBlockName":"Weather","ID":4,"PaperDescription":"Loads meteorological and weather data","Inputs":["Historical weather data"],"Outputs":["Structured weather dataset"],"ReferenceCitation":"Section 3.2: Meteorological and weather data are details about past weather conditions that can be found on historical websites.","FeedsInto":[7]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Map Matching","GranularBlocks":[{"GranularBlockName":"GPS Trajectory Points","ID":5,"PaperDescription":"Matches GPS trajectory points to road segments using ST-Matching","Inputs":["Trajectory points","Road network"],"Outputs":["Mapped road segment data"],"ReferenceCitation":"Section 3.3.1: The matching of the road network is performed using the ST-Matching algorithm, which considers topological information.","FeedsInto":[7]}]},{"IntermediateBlockName":"Embedding","GranularBlocks":[{"GranularBlockName":"Road Network Graph","ID":6,"PaperDescription":"Generates spatial representation of road networks","Inputs":["Road network graph"],"Outputs":["Road feature vector"],"ReferenceCitation":"Section 3.3.2: We employ the DeepWalk algorithm to produce an efficient spatial representation of the original road network.","FeedsInto":[7]}]},{"IntermediateBlockName":"Feature Extraction","GranularBlocks":[{"GranularBlockName":"Road Speed Factors","ID":7,"PaperDescription":"Constructs feature vectors for road speed prediction","Inputs":["Traffic data","Weather data","Air data"],"Outputs":["Feature matrix for model"],"ReferenceCitation":"Section 3.3.3: We construct a road feature vector as an input to the model for predicting short-term future road speed.","FeedsInto":[9]}]},{"IntermediateBlockName":"Indexing","GranularBlocks":[{"GranularBlockName":"Speed Performance","ID":8,"PaperDescription":"Computes road congestion level based on speed deviation","Inputs":["Speed data"],"Outputs":["Congestion index score"],"ReferenceCitation":"Section 3.3.4: The Road Speed Performance Index is utilized. This is calculated by dividing the actual vehicle speed by the maximum allowable travel speed.","FeedsInto":[14,15]}]},{"IntermediateBlockName":"Prediction","GranularBlocks":[{"GranularBlockName":"Road Speed","ID":9,"PaperDescription":"Predicts short-term traffic speed using a machine learning model","Inputs":["Road feature matrix"],"Outputs":["Predicted road speed"],"ReferenceCitation":"Section 4: We applied a model based on machine learning to estimate the short-term speed of roads in the future.","FeedsInto":[8,10]}]},{"IntermediateBlockName":"Quantify","GranularBlocks":[{"GranularBlockName":"Multi-source Factors on Congestion","ID":10,"PaperDescription":"Explains the influence of multi-source factors on congestion","Inputs":["Prediction results","Input data"],"Outputs":["SHAP values for feature impact"],"ReferenceCitation":"Section 4.1: We used the SHAP method to explain the speed prediction model and quantify the degree of influence of multi-source factors on congested roads.","FeedsInto":[11,12,13,17,18]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Bar Chart (Global View)","ID":11,"PaperDescription":"Displays overall impact of influencing factors","Inputs":["Aggregated factor importance"],"Outputs":["Bar and pie charts"],"ReferenceCitation":"Section 5.1: Global view showed basic data information, encoding the average degree of influence for multiple influencing factors through bar charts and pie charts.","FeedsInto":[19]},{"GranularBlockName":"Pie Chart","ID":12,"PaperDescription":"Displays overall impact of influencing factors","Inputs":["Aggregated factor importance"],"Outputs":["Bar and pie charts"],"ReferenceCitation":"Section 5.1: Global view showed basic data information, encoding the average degree of influence for multiple influencing factors through bar charts and pie charts.","FeedsInto":[19]},{"GranularBlockName":"Scatter Plot","ID":13,"PaperDescription":"Shows correlation between congestion factors and impact","Inputs":["Feature impact values"],"Outputs":["Scatterplot visualization"],"ReferenceCitation":"Section 5.2: Relationship view demonstrated the response relationship between influencing factors in terms of their original value domains and their degree of influence in a two-dimensional space.","FeedsInto":[19]},{"GranularBlockName":"Line Chart","ID":14,"PaperDescription":"Tracks temporal traffic congestion trends","Inputs":["Traffic data over time"],"Outputs":["Time-series congestion chart"],"ReferenceCitation":"Section 5.3: Monitor view supported exploring the occurrence of traffic congestion at different moments, with time-series charts representing congestion levels.","FeedsInto":[20]},{"GranularBlockName":"Matrix","ID":17,"PaperDescription":"Compares congestion impact of different roads","Inputs":["Road congestion factors"],"Outputs":["Color-encoded matrix"],"ReferenceCitation":"Section 5.5: Matrix view adopted a matrix design, where each row represented a road, and columns represented influencing factors with color encoding their impact.","FeedsInto":[20]},{"GranularBlockName":"Bar Chart (Temporal View)","ID":18,"PaperDescription":"Analyzes factor importance changes over time","Inputs":["Historical congestion data"],"Outputs":["Line chart with factor impact"],"ReferenceCitation":"Section 5.6: Temporal view was designed to analyze how the degree of influence of different factors on the speed of the target road changed over time.","FeedsInto":[19]}]},{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Overlay (Roads)","ID":15,"PaperDescription":"Overlays congestion levels on the map","Inputs":["Road congestion levels"],"Outputs":["Color-coded congestion overlay"],"ReferenceCitation":"Section 5.4: Mapview depicted traffic conditions on the road, using four colors to represent varying levels of congestion.","FeedsInto":[16,20]},{"GranularBlockName":"2D Map","ID":16,"PaperDescription":"Displays road network and locations","Inputs":["Road network data"],"Outputs":["Base map visualization"],"ReferenceCitation":"Section 5.4: Mapview represented the real geographic locations of roads and related attribute information.","FeedsInto":[20]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Factor Selection","ID":19,"PaperDescription":"Enables users to filter factors influencing congestion","Inputs":["User selection"],"Outputs":["Filtered congestion data"],"ReferenceCitation":"Section 6.1: Recognizing the different degree of influence could help traffic managers develop targeted measures to alleviate traffic congestion.","FeedsInto":[7]},{"GranularBlockName":"Road Selection","ID":20,"PaperDescription":"Allows users to select roads for detailed congestion analysis","Inputs":["User-selected road"],"Outputs":["Focused congestion details"],"ReferenceCitation":"Section 6.2: We investigated traffic congestion from monitor view by randomly selecting six times on several days.","FeedsInto":[9]}]}],"HighBlockName":"Interaction"}]},{"Year":2022,"PaperTitle":"Urban Rhapsody: Large-scale exploration of urban soundscapes","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Audio Recordings","ID":1,"PaperDescription":"Loads raw urban audio data (3x10s recordings per minute) from the SONYC sensor network","Inputs":["Sensor-collected audio files stored on disk"],"Outputs":["Raw audio files for subsequent feature extraction"],"ReferenceCitation":"Sec. 4.1: As of 2021, SONYC has collected... 877,000 hours of recorded audio.","FeedsInto":[3,11]},{"GranularBlockName":"Audio Embeddings","ID":2,"PaperDescription":"Loads OpenL3-generated audio embeddings (each 10s clip => 10 frames of 512-d vectors)","Inputs":["Precomputed embedding vectors (512-dim) for each 1-second audio frame"],"Outputs":["A memory-resident or queryable store of embedding vectors"],"ReferenceCitation":"Sec. 4.1: For each 10-second recording, we... produce 10 512-dimensional feature vectors...","FeedsInto":[4,5,6,7]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Audio Features","ID":3,"PaperDescription":"Transforms raw audio into 512-d embedding vectors using self-supervised audio model","Inputs":["Raw audio files"],"Outputs":["A 512-d embedding vector for each 1s frame (OpenL3 features)"],"ReferenceCitation":"Sec. 4.1: We employ OpenL3 [CWSB19]... for each snippet, we compute a feature vector...","FeedsInto":[4,5,6,7]}]},{"IntermediateBlockName":"Dimension Reduction","GranularBlocks":[{"GranularBlockName":"High-dimensional Audio Feature Vectors","ID":4,"PaperDescription":"Maps high-dimensional audio embeddings down to 2D for the Day View scatterplots","Inputs":["Subset of embedding vectors","User selection filters"],"Outputs":["2D coordinates for each frame, used to generate scatterplots"],"ReferenceCitation":"Sec. 6.2: In the Day View... we visualize the audio frames... by projecting them with UMAP [MHM18].","FeedsInto":[9]}]},{"IntermediateBlockName":"Clustering","GranularBlocks":[{"GranularBlockName":"Embedding Vectors","ID":5,"PaperDescription":"Generates a hierarchical tree of audio-frame clusters for the Mixture Explorer","Inputs":["Subset of embedding vectors for a single day"],"Outputs":["A cluster tree to help explore mixture of sound categories"],"ReferenceCitation":"Sec. 6.2: When a day is loaded, Urban Rhapsody automatically calculates a hierarchical clustering... for the mixture explorer.","FeedsInto":[10]}]},{"IntermediateBlockName":"Classification","GranularBlocks":[{"GranularBlockName":"Likelihood Scoring","ID":6,"PaperDescription":"Trains user-defined concept models from positively/negatively labeled frames","Inputs":["User-labeled audio frames","Negative samples"],"Outputs":["A per-concept binary classifier producing probability scores per frame"],"ReferenceCitation":"Sec. 6.1: We define prototypes... the classification model is trained via random forest using user-labeled frames...","FeedsInto":[8,10,12,14]}]},{"IntermediateBlockName":"Similarity Calculation","GranularBlocks":[{"GranularBlockName":"Euclidean Distance of Audio Features","ID":7,"PaperDescription":"Finds audio frames similar to a user-provided snippet or concept","Inputs":["User snippet or prototype concept","ANN index over embeddings"],"Outputs":["Retrieved set of top-k frames matching the snippet or concept"],"ReferenceCitation":"Sec. 6.3: We can use representative frames as query input for an approximated nearest neighbors (ANN) query...","FeedsInto":[8,9,14]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":13,"PaperDescription":"Shows sensor positions or city boundaries in a 2D map reference","Inputs":["Coordinates of sensors","City polygons"],"Outputs":["An interactive 2D map for basic geographic context"],"ReferenceCitation":"Sec. 4.1 / Fig. 2: The SONYC acoustic sensor network... distribution of the sensors... around three boroughs of NYC.","FeedsInto":[]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Calendar","ID":8,"PaperDescription":"Displays day-level distribution of frames or concept density over a year","Inputs":["Likelihood scores / concept counts aggregated by day"],"Outputs":["A color-coded calendar with bar charts per day/time block"],"ReferenceCitation":"Sec. 6.2: The first is the Calendar View... each cell representing a single day with bar charts...","FeedsInto":[17]},{"GranularBlockName":"Scatter Plot","ID":9,"PaperDescription":"Renders scatterplots of frames for a single day after UMAP projection (Day View)","Inputs":["Subset of embedding vectors (e.g., day's frames)","2D coordinates"],"Outputs":["A 2D scatterplot enabling cluster inspection, selection, labeling"],"ReferenceCitation":"Sec. 6.2: In the Day View... we visualize the audio frames... scatterplots... using UMAP.","FeedsInto":[16]},{"GranularBlockName":"Hierarchical Tree","ID":10,"PaperDescription":"Shows hierarchical clusters in the Mixture Explorer to find sound mixtures","Inputs":["Clustering results plus concept probability scores"],"Outputs":["A tree with subnodes color-coded by concept likelihood"],"ReferenceCitation":"Sec. 6.2: Mixture Explorer... we do hierarchical clustering... each node subdivided by user concepts...","FeedsInto":[20]},{"GranularBlockName":"Spectrogram","ID":11,"PaperDescription":"Displays selected audio snippet as a spectrogram (Focused View)","Inputs":["A subset of raw audio frames selected by user"],"Outputs":["Spectrogram plots in time-frequency space for frame analysis"],"ReferenceCitation":"Sec. 6.2: The Focused View shows a spectrogram of the audio samples... bridging visual representation and actual audio.","FeedsInto":[19]},{"GranularBlockName":"Matrix","ID":12,"PaperDescription":"Depicts concept-likelihood matrix for each 1-second frame of the snippet","Inputs":["User-labeled frames","Classification probabilities from prototypes"],"Outputs":["A matrix showing concept membership probabilities per frame"],"ReferenceCitation":"Sec. 6.2: Frame Classification View... displays the likelihood of observing a concept in the audio sample.","FeedsInto":[]},{"GranularBlockName":"Distribution Chart","ID":14,"PaperDescription":"Depicts the numeric distributions (e.g. hourly or daily frequency) of frames or concepts","Inputs":["Aggregated data for day/time","Concept frequency"],"Outputs":["Histogram or bar chart to quickly grasp distribution patterns"],"ReferenceCitation":"Sec. 6.2: ...the hour distribution chart... a bar chart representing how frames are spread across time slices...","FeedsInto":[]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Audio Query","ID":15,"PaperDescription":"Lets user pick or upload an audio snippet as input for similarity-based retrieval","Inputs":["User-provided snippet","Previously retrieved snippet"],"Outputs":["A set of frames returned as top matches to the snippet/concept"],"ReferenceCitation":"Sec. 6.3: The exploration process starts with the user querying... by selecting a frame from examples or uploading their snippet...","FeedsInto":[7]},{"GranularBlockName":"Bounding Box Selection","ID":16,"PaperDescription":"Allows user to drag a box over scatterplot to pick frames for inspection/labeling","Inputs":["2D scatterplot region","User's bounding box gesture"],"Outputs":["Selected frames pass to labeling or playback modules"],"ReferenceCitation":"Sec. 6.2: ...the scatterplots... allow selecting points through bounding box or period of day...","FeedsInto":[18]},{"GranularBlockName":"Temporal Selection","ID":17,"PaperDescription":"Enables user to click a day cell in the Calendar to load that day's audio frames","Inputs":["A day cell from the Calendar View"],"Outputs":["Loads and projects that day's frames in the Day View"],"ReferenceCitation":"Sec. 6.2: If a Calendar cell is clicked, all data for that day is loaded...","FeedsInto":[4]},{"GranularBlockName":"Tree Node Selection","ID":20,"PaperDescription":"Allows user to click a node in the hierarchical tree to select corresponding cluster in scatterplots","Inputs":["A tree node from the Mixture Explorer","User's click interaction"],"Outputs":["Selected cluster of frames highlighted in scatterplots and other views"],"ReferenceCitation":"Sec. 6.2: If a node is clicked, the corresponding cluster is selected in the scatterplots and all the components of the interface are updated accordingly.","FeedsInto":[9]}]},{"IntermediateBlockName":"Annotation","GranularBlocks":[{"GranularBlockName":"Labeling Audio Frames","ID":18,"PaperDescription":"Allows user to assign positive/negative labels for frames, building new concepts","Inputs":["Subsets of frames selected in scatterplots or cluster tree"],"Outputs":["Updated annotation sets that refine classification prototypes"],"ReferenceCitation":"Sec. 6.2: One of the system's requirements is the ability to annotate... user can label frames with as many labels as they want...","FeedsInto":[6]}]},{"IntermediateBlockName":"Playback","GranularBlocks":[{"GranularBlockName":"Listen to Spectrogram Snippet","ID":19,"PaperDescription":"Lets user click on a spectrogram to hear the snippet for deeper audio inspection","Inputs":["A spectrogram in Focused View","User's click on it"],"Outputs":["Audio playback in the browser for that snippet"],"ReferenceCitation":"Sec. 6.2: Users can click on the spectrogram to listen to the recording... bridging the gap between visual and actual audio.","FeedsInto":[]}]}],"HighBlockName":"Interaction"}]},{"Year":2014,"PaperTitle":"Run Watchers: Automatic Simulation-Based Decision Support in Flood Management","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"2D Features","ID":1,"PaperDescription":"Loads 2D footprints (e.g., building outlines, perimeter polygons) for flood planning","Inputs":["Footprint polygons from city data (e.g., shapefiles, OSM)"],"Outputs":["2D feature set for perimeter generation or object protection lines"],"ReferenceCitation":"Sec. 5: We define a perimeter as a closed line around important buildings uses footprints for geometry.","FeedsInto":[4,7]},{"GranularBlockName":"3D Geometries","ID":2,"PaperDescription":"Loads building extrusions / 3D shapes for deeper modeling (e.g., barrier placement, slopes)","Inputs":["3D building meshes","Extruded shapes"],"Outputs":["Scene geometry for precise barrier slope checks","Advanced 3D visualization"],"ReferenceCitation":"Sec. 5: We use a GPU-based flood simulation... building footprints extruded to 3D for barrier endpoints.","FeedsInto":[4,12]},{"GranularBlockName":"Digital Elevation Model","ID":3,"PaperDescription":"Imports the terrain DEM to compute water flow and slope constraints","Inputs":["Raster DEM (heights)","Triangular mesh of ground surface"],"Outputs":["In-memory DEM for shallow-water PDE","Slope-based barrier feasibility"],"ReferenceCitation":"Sec. 3.5: Barrier feasibility depends on maximum terrain slope so we rely on DEM.","FeedsInto":[4]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Simulation","GranularBlocks":[{"GranularBlockName":"Flood Scenario","ID":4,"PaperDescription":"Runs shallow-water PDE flood model for breach incidents, given initial geometry and barrier states","Inputs":["DEM","Building geometry","Breach scenario","Any placed barriers"],"Outputs":["Time-stepped outputs of water depth for each scenario/run","Time-stepped outputs of inundation wavefront for each scenario/run"],"ReferenceCitation":"Sec. 3.1: Every simulation run is a sequence of time steps for the breach scenario uses GPU-based PDE solution.","FeedsInto":[5,6,7,12]}]},{"IntermediateBlockName":"Simulation","GranularBlocks":[{"GranularBlockName":"Domain Rules","ID":5,"PaperDescription":"Implements watchers that spawn/terminate runs based on triggers (water detection, barrier fail, perimeter shrink)","Inputs":["Streaming PDE states","Domain rules (time constraints, barrier constraints)"],"Outputs":["Updated branching structure of runs (decision tree)","Reusing states"],"ReferenceCitation":"Sec. 3: Run Watchers program agents that manage multiple parallel simulation runs domain-specific rules for new or terminated runs.","FeedsInto":[4,6]}]},{"IntermediateBlockName":"Tree Construction","GranularBlocks":[{"GranularBlockName":"Decisions","ID":6,"PaperDescription":"Builds a cause-and-decision tree from watchers' time-step logs (water detection, barrier needed, etc.)","Inputs":["Logged events from watchers (time step, cause, decision)"],"Outputs":["A cause-effect tree linking all run branches into a single structure"],"ReferenceCitation":"Sec. 4: Run Watchers produce large decision trees we automatically extract them for explanation and visualization.","FeedsInto":[9,10,11]}]},{"IntermediateBlockName":"Calculation","GranularBlocks":[{"GranularBlockName":"Perimeter","ID":7,"PaperDescription":"Computes initial perimeter from wavefronts + important buildings, then shrinks or splits as barrier failures occur","Inputs":["Guaranteed response time wavefront","Final wavefront","Building footprints"],"Outputs":["A polygon perimeter (or multiple) that encloses the selected buildings, updated on segment removal"],"ReferenceCitation":"Sec. 3.4-3.6: We define perimeter as closed line, shrink it if segments fail segment removal leads to smaller polygon or object protection.","FeedsInto":[8]}]},{"IntermediateBlockName":"Evaluation","GranularBlocks":[{"GranularBlockName":"Geometry Positioning","ID":8,"PaperDescription":"Determines if a perimeter segment can be protected by a barrier or must be removed","Inputs":["Segment's water depth/slope","Watchers' barrier data (time, resource constraints)"],"Outputs":["Updated run configuration with chosen barrier type","Perimeter-segment removal"],"ReferenceCitation":"Sec. 3.5: If newly created barrier is overtopped next barrier type is tried. If none apply, remove segment.","FeedsInto":[5]}]},{"IntermediateBlockName":"Assessment","GranularBlocks":[{"GranularBlockName":"Protection Coverage","ID":9,"PaperDescription":"Shows why a certain building was left unprotected by enumerating perimeter attempts that failed","Inputs":["Building ID","Final decision tree results from watchers"],"Outputs":["Explanatory examples of perimeters that could have enclosed the building, plus cause logs"],"ReferenceCitation":"Sec. 4: Explaining unprotected building watchers provide reasons that this building was not included due to overshadowing more critical needs.","FeedsInto":[11]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Timeline","ID":10,"PaperDescription":"Groups watchers' decisions (barrier, perimeter, line-disable) into clusters along a horizontally oriented timeline","Inputs":["Decision tree logs: cause, time step, outcome, runs"],"Outputs":["Cluster-based timeline chart with color-coded partial or final survivors"],"ReferenceCitation":"Sec. 4.1: We propose a cluster-based timeline each cluster for a decision type re-scaling partial runs for clarity.","FeedsInto":[13]},{"GranularBlockName":"Storyboard","ID":11,"PaperDescription":"Generates storyboards: either final action plan, or entire chain of watchers' decisions, or hard decisions scenario","Inputs":["A chosen run/scenario","Bubset of runs relevant to an unprotected building"],"Outputs":["A horizontally oriented sequence of story blocks referencing each cause/decision, plus 3D snapshots"],"ReferenceCitation":"Sec. 4.2: Run Watchers automatically generate storyboards to present the chain of causes/decisions and final plan.","FeedsInto":[14]}]},{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"3D Scene","ID":12,"PaperDescription":"Displays water surface, building geometry, and any placed barriers at a chosen time step in 3D","Inputs":["Selected scenario/time from watchers","PDE water fields","Barrier lines"],"Outputs":["An interactive 3D environment with highlight on barrier or perimeter changes"],"ReferenceCitation":"Sec. 4.2: The thumbnail image is linked to main 3D view user can see barrier placement or perimeter updates in 3D.","FeedsInto":[13,14]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Scenario Selection","ID":13,"PaperDescription":"Clicking a time step or scenario in the cluster timeline picks that watchers' cause/decision for detail views","Inputs":["Mouse click on cluster timeline (run watchers' node)"],"Outputs":["Storyboard updates to that scenario chain, main 3D navigates to corresponding time"],"ReferenceCitation":"Sec. 4.3: Selecting a cause or decision in the cluster triggers an update of the storyboard & main 3D view facilitating full decision-tree navigation.","FeedsInto":[11,12]}]},{"IntermediateBlockName":"Manipulation","GranularBlocks":[{"GranularBlockName":"3D Geometry Replacement","ID":14,"PaperDescription":"Lets user spawn new PDE run from a given time step, overriding watchers (barrier or perimeter modifications)","Inputs":["Ongoing PDE state","user-chosen barrier","perimeter adjustments"],"Outputs":["A newly created run integrated into watchers' pipeline, possibly overshadowing watchers' default approach"],"ReferenceCitation":"Sec. 3.1: Users can manually spawn new scenario from any time step bridging what-if' questions with watchers' automation","FeedsInto":[5]}]}],"HighBlockName":"Interaction"}]},{"Year":2023,"PaperTitle":"PDViz: A Visual Analytics Approach for State Policy Data","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Policy Diffusion Data","ID":1,"PaperDescription":"Loads policy diffusion data from SPID and CSPP databases.","Inputs":["CSV files with policy adoption records and topics."],"Outputs":["Structured policy diffusion network data."],"ReferenceCitation":"Section 3.2.1 The policy diffusion is a network dataset, with each record describing which policy is spreading from one US state to another.","FeedsInto":[2,3]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Policy Diffusion Patterns","ID":2,"PaperDescription":"Infers policy diffusion patterns using the NetInf algorithm.","Inputs":["Policy adoption records as a network graph."],"Outputs":["Identified policy diffusion pathways."],"ReferenceCitation":"Section 3.2.1 We infer underlying policy diffusion patterns in the policy diffusion network... using the NetInf algorithm.","FeedsInto":[4,5,6,7,8,9,10,11]}]},{"IntermediateBlockName":"Regression","GranularBlocks":[{"GranularBlockName":"Probability of Policy Adoption","ID":3,"PaperDescription":"Uses a Cox proportional hazards model to predict policy adoption likelihood.","Inputs":["Policy adoption data with contextual factors."],"Outputs":["Probabilities of policy adoption across states."],"ReferenceCitation":"Section 3.2.4 The Cox model is the widely used method to model the probability of policy adoption following the context factors over time.","FeedsInto":[4,5,6,7,8,9,10,11]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Arc Diagram","ID":4,"PaperDescription":"Displays policy diffusion patterns as an arc diagram for comparative analysis.","Inputs":["Policy diffusion network graph."],"Outputs":["Arc diagram highlighting policy pathways."],"ReferenceCitation":"Section 3.3.2 PDViz adopts an arc diagram layout to present two different underlying patterns up and down for the user to compare them.","FeedsInto":[13]},{"GranularBlockName":"Choropleth Map","ID":5,"PaperDescription":"Visualizes policy adoption intensity across states using color.","Inputs":["Policy adoption data per state."],"Outputs":["A geographic map with intensity-based coloring."],"ReferenceCitation":"Section 3.3.3 The map view is designed to present measurement information with geographical information.","FeedsInto":[13]},{"GranularBlockName":"Hexbin","ID":6,"PaperDescription":"Provides a hexbin alternative to traditional maps to reduce distortion.","Inputs":["Policy adoption data per state."],"Outputs":["Hexbin map highlighting policy diffusion."],"ReferenceCitation":"Section 3.3.3 The map view provides two styles of maps: the general US state map and hexbin map.","FeedsInto":[13]},{"GranularBlockName":"Heat Matrix","ID":7,"PaperDescription":"Displays policy adoption counts across states and topics as a matrix.","Inputs":["Policy adoption counts."],"Outputs":["A heat matrix with state-policy adoption intensities."],"ReferenceCitation":"Section 3.3.2 The policy matrix chart shows the heatmap style overview representing how many new and existing policies have been created and adopted by states.","FeedsInto":[13]},{"GranularBlockName":"Dot Plot","ID":8,"PaperDescription":"Highlights state policy adoption frequency across categories.","Inputs":["Policy adoption data per state and topic."],"Outputs":["Dot plot visualizing adoption distributions."],"ReferenceCitation":"Section 3.3.4 The By Context tab provides a line-box chart to allow the user to observe policy adoptions along with social factors.","FeedsInto":[]},{"GranularBlockName":"Timeline","ID":9,"PaperDescription":"Displays policy adoption trends over time per state and topic.","Inputs":["Time-series policy adoption data."],"Outputs":["A timeline chart showing policy adoption history."],"ReferenceCitation":"Section 3.3.4 The By Year tab shows the number of policies that are newly created or adopted over time.","FeedsInto":[]},{"GranularBlockName":"Line Chart","ID":10,"PaperDescription":"Analyzes social and political factors over time alongside policy adoptions.","Inputs":["Contextual factor trends."],"Outputs":["Line chart overlaying policy adoption with external factors."],"ReferenceCitation":"Section 3.3.4 The By Context tab provides a line chart representing social factors.","FeedsInto":[]},{"GranularBlockName":"Bar Chart","ID":11,"PaperDescription":"Shows policy adoption statistics per topic and state.","Inputs":["Policy adoption data per category."],"Outputs":["A bar chart ranking policy adoption rates."],"ReferenceCitation":"Section 3.3.4 The By Topic tab is a bar chart showing the number of unique policies by topic.","FeedsInto":[]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Policy Selection","ID":12,"PaperDescription":"Allows users to select and filter policies for analysis.","Inputs":["User selection from dropdown."],"Outputs":["Filtered policy adoption visualizations."],"ReferenceCitation":"Section 3.3.1 The drop-down buttons allow the user to select the topic, year ranges, and sorting options.","FeedsInto":[2,3]},{"GranularBlockName":"Mouse Hover","ID":13,"PaperDescription":"Highlights states and policies with contextual information on hover.","Inputs":["User hover action."],"Outputs":["Tooltips with detailed policy adoption data."],"ReferenceCitation":"Section 3.3.2 When the user hovers the mouse over a policy label, a tooltip appears showing contextual factors along with associated Coxs hazard ratios.","FeedsInto":[2,3]}]}],"HighBlockName":"Interaction"}]},{"Year":2017,"PaperTitle":"Urban Pulse: Capturing the Rhythm of Cities","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Spatiotemporal Images Dataset","ID":1,"PaperDescription":"Loads Flickr data including image metadata (timestamps and geolocations) to capture the dynamic events of the city","Inputs":["Raw Flickr data (images with spatiotemporal metadata)"],"Outputs":["Structured spatiotemporal dataset for density function computation"],"ReferenceCitation":"All the data sets used in this paper are composed of a set of data points over space and time. (Introduction / Section 3)","FeedsInto":[3]},{"GranularBlockName":"2D Features","ID":2,"PaperDescription":"Loads a planar mesh representing the city's spatial domain","Inputs":["Raw geographic data (e.g., city boundaries)"],"Outputs":["Triangulated 2D mesh representing the city's spatial features"],"ReferenceCitation":"Mesh representation of a city. Given a city, the mesh corresponding to it represents the bounding box... (Mesh Representation section)","FeedsInto":[9,10]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Prominent Density Feature Identification","ID":3,"PaperDescription":"Aggregates high-persistence topological features to identify prominent locations based on high density focusing on the computed density rather than using the author's pulse terminology","Inputs":["High-persistence topological features"],"Outputs":["Identified prominent density features (i.e. locations with high density)"],"ReferenceCitation":"a prominent location is a high persistent maximum of a function... (Section 4.1)","FeedsInto":[4,5,6,9]},{"GranularBlockName":"Significant Density Profile","ID":4,"PaperDescription":"Derives a binary (0/1) sequence indicating, for each time step, whether a significant (high-persistence) maximum is present at the location","Inputs":["Density functions at the identified locations"],"Outputs":["Binary sequence (significant density profile) per location"],"ReferenceCitation":"Significant Beats Bs: This is a 0/1 sequence indicating the absence/presence of a high persistent maximum... (Section 4.2 adapted)","FeedsInto":[7]},{"GranularBlockName":"Maxima Density Profile","ID":5,"PaperDescription":"Computes a binary sequence that records, for each time step, whether any local maximum is present at the location, regardless of persistence","Inputs":["Density functions at the identified locations"],"Outputs":["Binary sequence (maxima density profile) per location"],"ReferenceCitation":"Maxima Beats Bm: This is a 0/1 sequence indicating the absence/presence of a maximum at the location... (Section 4.2 adapted)","FeedsInto":[7]},{"GranularBlockName":"Density Function Profile","ID":6,"PaperDescription":"Generates a continuous time series capturing the variation of the density function value at the location, representing the magnitude of density over time","Inputs":["Density functions at the identified locations"],"Outputs":["Continuous time series (density function profile) per location"],"ReferenceCitation":"Function Beats Bf: This is a time series showing the variation of the scalar function at that location... (Section 4.2 adapted)","FeedsInto":[7,12]}]},{"IntermediateBlockName":"Ranking","GranularBlocks":[{"GranularBlockName":"High-dimensional Density Feature Vector","ID":7,"PaperDescription":"Constructs a high-dimensional feature vector from the three density profiles (significant, maxima, and function) and computes an L2-norm rank to quantify overall density intensity","Inputs":["Combined density profiles across all resolutions"],"Outputs":["Numeric rank score derived from the L2 norm of the density feature vector"],"ReferenceCitation":"For a given pulse P, its rank is computed as the L2 norm of its corresponding feature vector... (Section 4.3 adapted as density feature vector)","FeedsInto":[8,10,11]}]},{"IntermediateBlockName":"Similarity Calculation","GranularBlocks":[{"GranularBlockName":"Euclidean Distance of Density Feature Vectors","ID":8,"PaperDescription":"Calculates similarity between locations by computing the Euclidean distance between their high-dimensional density feature vectors","Inputs":["High-dimensional density feature vectors"],"Outputs":["Similarity measure (L2 norm distance) between density features"],"ReferenceCitation":"Pulse Similarity is defined as the L2 norm of the similarity vector computed from the differences between beats... (Section 4.3 adapted to density feature vectors)","FeedsInto":[11]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Map 2D","ID":9,"PaperDescription":"Renders a 2D map that displays the spatial regions corresponding to the identified density feature locations, optionally overlaying a heatmap of the computed density values","Inputs":["Density feature location data","2D mesh geometry"],"Outputs":["2D map visualization"],"ReferenceCitation":"Map widget. The map view component consists of one or more map widgets... (Section 6 adapted as Map 2D)","FeedsInto":[14,15]},{"GranularBlockName":"Overlay (Heatmap)","ID":10,"PaperDescription":"Renders a heatmap overlay on the 2D map to visually represent the computed density values across different spatial locations","Inputs":["Computed density values","2D spatial features (mesh)"],"Outputs":["2D heatmap overlay integrated with the map view"],"ReferenceCitation":"optional heat map overlay of the density function. (Section 6 adapted)","FeedsInto":[9]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Scatter Plot","ID":11,"PaperDescription":"Visualizes density ranking data using a scatter plot, enabling comparison of density features across different temporal resolutions","Inputs":["Density rank scores"],"Outputs":["Scatter plot showing density feature rankings"],"ReferenceCitation":"Linked scatter plots. The default mode of the Pulse Monitor consists of three linked scatter plots... (Section 6 adapted as Scatter Plot)","FeedsInto":[13]},{"GranularBlockName":"Line Chart","ID":12,"PaperDescription":"Displays the density function profile as a line chart plotting time steps against density values to illustrate temporal variations at a location","Inputs":["Density function profile (continuous time series)"],"Outputs":["Line chart visualizing temporal variation of density magnitudes"],"ReferenceCitation":"The pulse beat viewer visualizes function beats as a line plot... (Section 6.1 adapted as Line Chart)","FeedsInto":[]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Plot Brush","ID":13,"PaperDescription":"Enables users to filter density feature locations by brushing over the scatter plot, linking the selection to updates across all visualizations","Inputs":["User brush input on the scatter plot","Density rank data"],"Outputs":["Filtered density feature locations highlighted in both scatter plot and map view"],"ReferenceCitation":"Pulse filtering and exploration. Users can select pulses of interest by brushing one of the scatter plots... (Section 6.2 adapted as Scatter Plot Brush)","FeedsInto":[7]},{"GranularBlockName":"Area Selection","ID":14,"PaperDescription":"Allows users to directly select a geographic area on the map to filter and display density feature locations within that area","Inputs":["User-drawn geographic area","Density feature location"],"Outputs":["Density features within the selected area are highlighted/filtered"],"ReferenceCitation":"The map view component provides spatial context for pulses... (Section 6 inferred for Area Selection)","FeedsInto":[3]},{"GranularBlockName":"Pulse Selection","ID":15,"PaperDescription":"Queries for similar pulses based on similarity measures and user input returns a ranked list of similar pulses for visualization","Inputs":["Similarity measures","A user-selected query pulse"],"Outputs":["Ranked list of similar pulses to update visualizations (e.g., scatter plot)"],"ReferenceCitation":"...similarity and vice versa. For example, consider the pulse corresponding to Alcatraz Island in San Francisco (SF) shown in Fig. 5... Section 4.3","FeedsInto":[7]}]}],"HighBlockName":"Interaction"}]},{"Year":2024,"PaperTitle":"SenseMap: Urban Performance Visualization and Analytics Via Semantic Textual Similarity","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"POI Data","ID":1,"PaperDescription":"Loads raw POI data from Baidu Maps for Shanghai","Inputs":["Raw POI data"],"Outputs":["Structured POI dataset with class info"],"ReferenceCitation":"Sec. IV-A: We focus on the POI data of Shanghai, China...","FeedsInto":[3]},{"GranularBlockName":"Traffic Analysis Zones","ID":2,"PaperDescription":"Loads TAZ derived from OpenStreetMap road networks","Inputs":["OpenStreetMap road network data"],"Outputs":["Defined TAZ boundaries as polygons"],"ReferenceCitation":"Sec. IV-A: We delineate the TAZs from road network data...","FeedsInto":[6]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filtering","GranularBlocks":[{"GranularBlockName":"POI Data","ID":3,"PaperDescription":"Removes unnecessary POI categories","Inputs":["Raw POI dataset"],"Outputs":["Filtered POI dataset"],"ReferenceCitation":"Sec. IV-A: We exclude POIs with minor influence areas...","FeedsInto":[4]}]},{"IntermediateBlockName":"Classification","GranularBlocks":[{"GranularBlockName":"POI Taxonomy","ID":4,"PaperDescription":"Organizes POIs into 12 thematic categories","Inputs":["Filtered POI dataset"],"Outputs":["Categorized POIs"],"ReferenceCitation":"Sec. IV-A: We establish our taxonomy following the scheme...","FeedsInto":[5]}]},{"IntermediateBlockName":"Similarity Calculation","GranularBlocks":[{"GranularBlockName":"POI-Measure Relationship","ID":5,"PaperDescription":"Computes semantic similarity between POIs and urban performance measures","Inputs":["POI dataset","Predefined urban measures","Textual corpora"],"Outputs":["POI contribution scores"],"ReferenceCitation":"Sec. IV-B: To quantify the contribution of different POIs...","FeedsInto":[6]}]},{"IntermediateBlockName":"KDE","GranularBlocks":[{"GranularBlockName":"POI + Semantic Scores","ID":6,"PaperDescription":"Extends KDE by adjusting kernel bandwidths","Inputs":["POI dataset","Semantic scores"],"Outputs":["Semantic density maps"],"ReferenceCitation":"Sec. IV-D: We propose SAKDE as an improvement of KDE...","FeedsInto":[7,8,9]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":7,"PaperDescription":"Displays the semantic maps overlaid on a base map","Inputs":["OpenStreetMap data"],"Outputs":["2D map visualization"],"ReferenceCitation":"Sec. V-C: Map View (Fig. 7(a)) shows the semantic maps overlaid on the base map after users select the measure...","FeedsInto":[10,11]},{"GranularBlockName":"Overlay (Heatmap)","ID":8,"PaperDescription":"Generates a heatmap overlay of urban performance","Inputs":["Semantic density maps","User-selected measures"],"Outputs":["Color-coded heatmap"],"ReferenceCitation":"Sec. V-C: Map View shows the semantic maps overlaid...","FeedsInto":[7,10,11]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Radial Bar Chart","ID":9,"PaperDescription":"Displays POI category distributions within a selected region","Inputs":["Aggregated POI data","Measure scores"],"Outputs":["Circular bar chart showing category proportions"],"ReferenceCitation":"Sec. V-C: The score scale from 0 to 100 is displayed at the center of the radial bar chart","FeedsInto":[10,11]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"POI Selection","ID":10,"PaperDescription":"Allows users to refine displayed POIs","Inputs":["User-defined category filters"],"Outputs":["Filtered POIs"],"ReferenceCitation":"Sec. V-C: The system supports interactions such as filtering POIs...","FeedsInto":[6]},{"GranularBlockName":"Area Selection","ID":11,"PaperDescription":"Allows users to define a region of interest","Inputs":["User-defined geographic selection"],"Outputs":["Computed urban performance for the region"],"ReferenceCitation":"Sec. V-C: Circle selection enables radius adjustments...","FeedsInto":[6]}]}],"HighBlockName":"Interaction"}]},{"Year":2024,"PaperTitle":"Submerse: Visualizing Storm Surge Flooding Simulations in Immersive Display Ecologies","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Flood Simulation Data","ID":1,"PaperDescription":"Loads time-series flood simulation data, including water elevation and tidal velocity.","Inputs":["Time-series data of water elevation","Tidal velocity, discretized over a geographic grid"],"Outputs":["Structured flood simulation dataset for visualization and analysis."],"ReferenceCitation":"Sec. 3.1: The flood simulation data, which serves as input to our application, is a time series data of scalar water elevation values and 2D tidal velocity, discretized over a vast irregular grid of latitude and longitude positions.","FeedsInto":[5,7]},{"GranularBlockName":"3D Geometries","ID":2,"PaperDescription":"Loads 3D building models and additional reference objects for flood simulation.","Inputs":["3D building models","Reference objects"],"Outputs":["Structured 3D environment for integration with flood visualization."],"ReferenceCitation":"Sec. 3.2: ...supplementary scene objects, such as 3D buildings and reference objects.","FeedsInto":[9,10]},{"GranularBlockName":"Digital Elevation Model","ID":3,"PaperDescription":"Loads terrain elevation data to integrate with the flood simulation environment.","Inputs":["Digital Elevation Models (DEM)."],"Outputs":["3D terrain representation for flood overlay."],"ReferenceCitation":"Sec. 3.2: The virtual scene is additionally set up using GIS data, such as digital elevation map (DEM).","FeedsInto":[5,9,10]},{"GranularBlockName":"Satellite Imagery","ID":4,"PaperDescription":"Loads high-resolution satellite imagery to enhance visualization of flood-prone areas.","Inputs":["Satellite imagery datasets."],"Outputs":["Integrated high-resolution texture layers for terrain and buildings."],"ReferenceCitation":"Sec. 3.2: The virtual scene is additionally set up using GIS data, such as digital elevation map (DEM) and satellite imagery.","FeedsInto":[9,10]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Interpolation","GranularBlocks":[{"GranularBlockName":"Water Surface Reconstruction","ID":5,"PaperDescription":"Reconstructs a continuous water surface using a quadtree grid","Inputs":["Sparse water depth data"],"Outputs":["Smooth water surface mesh using bilinear interpolation"],"ReferenceCitation":"Section IV-B: We discretize the data on a quadtree and interpolate the values...","FeedsInto":[7,8]}]},{"IntermediateBlockName":"Ray Casting","GranularBlocks":[{"GranularBlockName":"POI Visibility","ID":6,"PaperDescription":"Determines best viewpoints for flood visualization","Inputs":["List of POIs","Camera constraints"],"Outputs":["Optimized camera path minimizing occlusions"],"ReferenceCitation":"Section VI: The system should assist with providing effective views...","FeedsInto":[12]}]},{"IntermediateBlockName":"Rasterization","GranularBlocks":[{"GranularBlockName":"Water Flow","ID":7,"PaperDescription":"Animates water flow using GPU rasterization","Inputs":["Tidal velocity vectors","Water surface"],"Outputs":["Realistic animated flow visualization"],"ReferenceCitation":"Section IV-B: We apply Gerstner wave synthesis model using velocity vectors...","FeedsInto":[8]},{"GranularBlockName":"Flood Simulation","ID":8,"PaperDescription":"GPU-based rasterization of flood simulation data for real-time rendering.","Inputs":["Large flood simulation datasets."],"Outputs":["Interactive, real-time flood visualizations on immersive displays."],"ReferenceCitation":"Sec. 4.3: Our algorithms are designed to support the efficient rendering of large simulation data at interactive frame rates and to be scalable for distributed immersive visual display systems.","FeedsInto":[9,10,11]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"3D Scene","ID":9,"PaperDescription":"Integrates flooding data into a 3D virtual model for an immersive experience.","Inputs":["Time-series flood simulation data","3D geospatial data"],"Outputs":["A to-scale, interactive 3D flood visualization."],"ReferenceCitation":"Sec. 5.1: We present an interactive visualization of the flooding scenario by integrating the flooding level into a to-scale 3D virtual model of the corresponding geographic area.","FeedsInto":[13,14]},{"GranularBlockName":"Overlay (Flood)","ID":10,"PaperDescription":"Overlays flood simulation on 3D topographic models","Inputs":["Flood simulation","3D terrain","Buildings"],"Outputs":["Animated flood extent over time"],"ReferenceCitation":"Section IV-D: We present an interactive visualization of the flooding scenario...","FeedsInto":[9,13,14]}]},{"IntermediateBlockName":"Augmented Reality","GranularBlocks":[{"GranularBlockName":"Focus+Context","ID":11,"PaperDescription":"Augments immersive displays with localized flood severity views","Inputs":["Camera-tracked user device","Flood simulation data"],"Outputs":["AR-based personalized flood visualization"],"ReferenceCitation":"Section IV-C: We introduce a mixed-reality focus+context technique...","FeedsInto":[13,14]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Navigation","GranularBlocks":[{"GranularBlockName":"Camera Path Planning","ID":12,"PaperDescription":"Automates smooth camera transitions between POIs","Inputs":["User-selected POI","Occlusion constraints"],"Outputs":["Optimized path minimizing VR sickness"],"ReferenceCitation":"Section VI: We generate a smooth camera path for navigating to the POI views...","FeedsInto":[6]}]},{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"POI Selection","ID":13,"PaperDescription":"Enables users to interactively select flood impact areas","Inputs":["User-selected POI on The camera path is pre-calculated,3D map"],"Outputs":["Highlighted region with flood severity metrics"],"ReferenceCitation":"Section IV-D: Users can point and select specific buildings to view additional info...","FeedsInto":[6,12]}]},{"IntermediateBlockName":"Collaboration","GranularBlocks":[{"GranularBlockName":"Multi-User Display Synchronization","ID":14,"PaperDescription":"Synchronizes multi-GPU immersive display environments for collaborative flood analysis.","Inputs":["Multiple users interacting with a tiled display or immersive facility."],"Outputs":["Synchronized views across multiple displays."],"ReferenceCitation":"Section 6.2: Submerse manages synchronized instances of the application for each viewport in the system and optimizes computational and memory resources by determining view-dependent data extents.","FeedsInto":[6,13]}]}],"HighBlockName":"Interaction"}]},{"Year":2015,"PaperTitle":"Urbane: A 3D Framework to Support Data Driven Decision Making in Urban Development","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"2D Features","ID":1,"PaperDescription":"2D Data Layer Handling","Inputs":["Polygon","Line","Point","Grid data (e.g., roads, city neighborhoods) with 2D discrete geometry or aggregated numeric attributes"],"Outputs":["In-memory 2D features for subsequent analysis","Visualizations"],"ReferenceCitation":"Section 5: We support four types of 2D data layers - point layer, line layer, polygon layer, and grid layer","FeedsInto":[3]},{"GranularBlockName":"3D Geometries","ID":2,"PaperDescription":"3D Data Layer Handling","Inputs":["3D building meshes (parametric or triangle-based)","3D terrain","Discrete geometry with continuous coordinates","Urbane uses parametric meshes from Open Street Map for most buildings and triangle meshes for detailed landmarks"],"Outputs":["Structured 3D data for visualization and computations"],"ReferenceCitation":"Section 5: We classify the data layers as 2D and 3D layers, which can be either pre-computed or dynamically derived. [...] For the rest of the buildings, we use the parametric meshes also obtained from Open Street Map. [...] They are used to model buildings having a high level of detail such as the different landmarks..","FeedsInto":[3]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Indexing","GranularBlocks":[{"GranularBlockName":"Spatial Data","ID":3,"PaperDescription":"Spatial Indexing (kd-tree)","Inputs":["2D or 3D geometry data, typically discrete polygons or meshes, plus indexing parameters"],"Outputs":["A kd-tree or similar structure enabling fast spatial queries for rendering & analysis"],"ReferenceCitation":"Section 5: In order to enable fast retrieval of the data for the rendering as well as computation purposes, we index the data layers using the kd-tree data structure [5]","FeedsInto":[4,5,6,8,9]}]},{"IntermediateBlockName":"Rasterization","GranularBlocks":[{"GranularBlockName":"Landmark","ID":4,"PaperDescription":"Landmark Visibility Measure","Inputs":["3D geometry (buildings + a landmark)","Vantage points/rays","A rasterization approach"],"Outputs":["Per-building numeric coverage or visibility measures (continuous)"],"ReferenceCitation":"Section 6: We allow the user to inspect the impact of a new construction with respect to two quantitative criteria, namely, landmark visibility and sky exposure (illustrated in Fig. 3). [...] We only have to compute the views from different points of the landmark","FeedsInto":[7,10]},{"GranularBlockName":"Sky","ID":5,"PaperDescription":"Sky Exposure Measure","Inputs":["3D geometry (streets + buildings)","Discrete sample points (center lines along streets)","Upward vantage directions"],"Outputs":["Numeric fraction of visible sky at each street segment (continuous measure)"],"ReferenceCitation":"Section 6: Sky exposure. The streets of the city are divided into line segments of equal size, and the sky exposure is computed at the center points of these line segments. [...] We allow the user to inspect the impact of a new construction with respect to sky exposure","FeedsInto":[7,10]},{"GranularBlockName":"Coverage","ID":6,"PaperDescription":"GPU-Based Rasterization & Pixel Counting","Inputs":["3D geometry","Camera transform / color-encoding scheme","Discrete pass to compute coverage or occlusion"],"Outputs":["Pixel-based coverage data or occlusion metrics (continuous counts)"],"ReferenceCitation":"Section 6.3: In order to perform this computation in real time, we make use of the newly introduced compute shader that has been included as part of the graphics pipeline [17]. [...] The expensive aspect of this step is in retrieving the rendered scene from the GPU and performing pixel counting","FeedsInto":[7,10]}]},{"IntermediateBlockName":"Aggregation","GranularBlocks":[{"GranularBlockName":"Building/Region Metrics","ID":7,"PaperDescription":"Computing averages or min/max for user-selected sets","Inputs":["Multi-attribute building/region records","User's current selection (brush or threshold)"],"Outputs":["Numeric metrics (e.g. average building height, min or max values) to feed into the PCC or table"],"ReferenceCitation":"Section 7: We also visualize the attributes corresponding to the average of the items being shown. [...] The main exploration workflow supported in Urbane consists in exploring the data at the neighborhood level and later drilling-down to the building level","FeedsInto":[11,12]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":8,"PaperDescription":"Map View (2D Mode)","Inputs":["2D geometry layers + numeric attributes","User viewpoint or navigation"],"Outputs":["A top-down orthographic map (pixels)"],"ReferenceCitation":"Section 7: We support two possible states of map rendering - 2D and 3D. This state is used to visualize the 2D data layers. For example, a top view of the map is shown similar to conventional GIS map interfaces","FeedsInto":[13]},{"GranularBlockName":"3D Scene","ID":9,"PaperDescription":"Map View (3D Mode)","Inputs":["3D building geometry","User camera transforms","Possible 2D overlays for data (heat-map, lines, etc.)"],"Outputs":["On-screen 3D environment (continuous 3D coords) displaying buildings, roads, etc"],"ReferenceCitation":"Section 7: The 3D state visualizes both 2D and 3D data layers. [...] For example, in Fig. 4(c), the 2D layers representing physical aspects of the city are shown together with a heat-map denoting the sky exposure over the road network. Transparency on the 3D layers can be used to avoid occlusion","FeedsInto":[13,14]},{"GranularBlockName":"Overlay (Heatmap)","ID":10,"PaperDescription":"Color-coded or heat-map style overlay","Inputs":["Aggregated numeric measure (e.g., sky exposure per road)","Geometry for roads or polygons"],"Outputs":["A color-coded overlay on the 2D","3D map reflecting intensity distribution"],"ReferenceCitation":"Section 7: For example, in Fig. 4(c), the 2D layers representing physical aspects of the city are shown together with a heat-map denoting the sky exposure over the road network","FeedsInto":[8,9,13]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Parallel Coordinates Plot","ID":11,"PaperDescription":"Parallel Coordinates Chart (PCC)","Inputs":["Multi-attribute numeric data (e.g., building metrics)","User brushing","Discrete category fields possible"],"Outputs":["Parallel-coordinates plot (pixels) for attribute comparison and filtering"],"ReferenceCitation":"Section 7: Each qualitative 2D data layer corresponds to one dimension in the PCC. [...] We also visualize the attributes corresponding to the average of the items being shown. [...] The main exploration workflow supported in Urbane consists in exploring the data at the neighborhood level and later drilling-down to the building level","FeedsInto":[13]},{"GranularBlockName":"Table","ID":12,"PaperDescription":"Data Table","Inputs":["Subset of discrete or numeric data records (e.g., building info, neighborhood polygons, attributes)"],"Outputs":["A tabular listing of attributes (pixels in table form)"],"ReferenceCitation":"Section 7: The filtered entities are listed in the data table and are also highlighted on the map view","FeedsInto":[13]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Geometry Selection","ID":13,"PaperDescription":"Filtering & Selection","Inputs":["Data sets with discrete/continuous attributes","User thresholds","Brushing"],"Outputs":["A subset of data that meets the constraints, triggers highlight or focus in the visuals"],"ReferenceCitation":"Section 7: The Data Exploration View can be used to select and filter entities having the required range of values along different data sets. [...] The buildings remaining after this filter is applied..","FeedsInto":[4,5,6,7]}]},{"IntermediateBlockName":"Manipulation","GranularBlocks":[{"GranularBlockName":"3D Geometry Replacement","ID":14,"PaperDescription":"Building Replacement (Change button)","Inputs":["Original 3D geometry (discrete building shapes)","User-chosen new geometry to place"],"Outputs":["Updated geometry set (some building replaced or a new shape inserted), re-triggering relevant computations"],"ReferenceCitation":"Section 7: Once a building of interest is chosen, the user can replace it with a new mesh using the Change button [...]. This operation will trigger the impact analysis computation","FeedsInto":[4,5,6]}]}],"HighBlockName":"Interaction"}]},{"Year":2022,"PaperTitle":"DDLVis: Real-time Visual Query of Spatiotemporal Data Distribution via Density Dictionary Learning","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Spatiotemporal Sensor Data","ID":1,"PaperDescription":"Loads air pollution and urban mobility sensor data for analysis","Inputs":["Sensor readings (PM2.5, CO, NO2, SO2, vehicle locations)","timestamps","GPS locations"],"Outputs":["Indexed spatiotemporal dataset"],"ReferenceCitation":"Section 3: Spatiotemporal data are increasingly produced in real-time by physical sensors, such as vehicles, mobile phones, and climate monitors.","FeedsInto":[2]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"KDE","GranularBlocks":[{"GranularBlockName":"Spatiotemporal Events","ID":2,"PaperDescription":"Computes density maps by estimating distributions of sensor readings over space & time","Inputs":["Raw spatiotemporal data (sensor readings, GPS)"],"Outputs":["Density maps with varying kernel sizes"],"ReferenceCitation":"Section 4: The peak-based kernel density estimation (PKDE) approach is presented...","FeedsInto":[3,4,5,6,7]}]},{"IntermediateBlockName":"Compression","GranularBlocks":[{"GranularBlockName":"Density Maps","ID":3,"PaperDescription":"Reduces memory usage by compressing density maps while preserving trends","Inputs":["Density maps over time"],"Outputs":["Compressed sparse-coded density maps"],"ReferenceCitation":"Section 5: We present a novel learning method, density dictionary learning (DDL)...","FeedsInto":[8,10,11,12]}]},{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Density Maps Features","ID":4,"PaperDescription":"Generates sparse-coded representations of density maps to highlight important patterns","Inputs":["Density maps"],"Outputs":["Compressed sparse-coded density patterns"],"ReferenceCitation":"Section 5.3: ASC method guarantees high accuracy of sparse representation...","FeedsInto":[12]}]},{"IntermediateBlockName":"Clustering","GranularBlocks":[{"GranularBlockName":"TSG on Density Maps","ID":5,"PaperDescription":"Identifies representative density maps to train the dictionary model efficiently","Inputs":["Density maps"],"Outputs":["Clustered representative training dataset"],"ReferenceCitation":"Section 5.1: We select representative density maps as the training set...","FeedsInto":[3]}]},{"IntermediateBlockName":"Tracking","GranularBlocks":[{"GranularBlockName":"Pixel Density Evolution","ID":6,"PaperDescription":"Captures temporal density variations by tracking pixel trends","Inputs":["Sequential density maps"],"Outputs":["Time-series representation of pixel variations"],"ReferenceCitation":"Section 5.4: We define the pixel in the same 2D position on sequential density maps as a pixel chain...","FeedsInto":[10,11]}]},{"IntermediateBlockName":"Detection","GranularBlocks":[{"GranularBlockName":"Direction & Magnitude Changes Density Maps","ID":7,"PaperDescription":"Computes spatial variation fields by detecting shifts in density patterns over time","Inputs":["Consecutive density maps"],"Outputs":["Direction & magnitude variation maps"],"ReferenceCitation":"Section 5.5: We generate the spatial variation field for each pair of adjacent density maps...","FeedsInto":[9]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":8,"PaperDescription":"Displays density distributions of spatiotemporal data using color gradients","Inputs":["2D Layer"],"Outputs":["2D map view"],"ReferenceCitation":"Section 4: PKDE ensures the performance of density map generation...","FeedsInto":[14]},{"GranularBlockName":"Overlay (Heatmap)","ID":9,"PaperDescription":"Visualizes spatial variation fields between density maps using arrows & color-coding","Inputs":["Spatial density variation data"],"Outputs":["Directional arrows","color-coded magnitude map"],"ReferenceCitation":"Section 5.5: We convert the spatial variation field into a direction map and an absolute map...","FeedsInto":[8]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Stream Graph","ID":10,"PaperDescription":"Represents temporal variations in density data over time","Inputs":["Temporal density dataset"],"Outputs":["Stream graph with trend visualization"],"ReferenceCitation":"Section 6.5: The stream view is a stream-based design to show the data evolution in a period...","FeedsInto":[13]},{"GranularBlockName":"Area Chart","ID":11,"PaperDescription":"Displays aggregated density trends over time","Inputs":["Temporal density dataset"],"Outputs":["Area chart showing cumulative trends"],"ReferenceCitation":"Section 6.5: The statistical view visualizes density distributions over different time ranges...","FeedsInto":[13]},{"GranularBlockName":"Bar Chart","ID":12,"PaperDescription":"Displays statistical summaries of density variations as horizontal bars","Inputs":["Query results"],"Outputs":["Horizontal bar chart with mean and max values"],"ReferenceCitation":"Section 6.5: We design a new statistical bar that can present the maximum and mean values...","FeedsInto":[13]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Temporal Selection","ID":13,"PaperDescription":"Lets users refine queries by selecting a specific time period to analyze density distributions.","Inputs":["User-selected time range"],"Outputs":["Filtered density maps"],"ReferenceCitation":"Section 6: The query period indicates the time range...","FeedsInto":[2]},{"GranularBlockName":"Area Selection","ID":14,"PaperDescription":"Allows users to interactively select a specific geographic area for focused analysis.","Inputs":["User-selected region"],"Outputs":["Filtered dataset & updated visualization"],"ReferenceCitation":"Section 6: Spatial query means the operation is applied on a geographical map...","FeedsInto":[2]}]}],"HighBlockName":"Interaction"}]},{"Year":2023,"PaperTitle":"TriPlan: an interactive visual analytics approach for better tourism route planning","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Travel Routes","ID":1,"PaperDescription":"Collects travel route data from online sources.","Inputs":["Travel routes from Qyer.com","Travel routes from Mafengwo.com"],"Outputs":["Structured route database for analysis and visualization"],"ReferenceCitation":"Section 3.2.1 We crawl 30,000 domestic city-level routes and 70,000 POI-level routes, including attributes such as locations, planned travel duration, time spent, page views, and the number of times copied.","FeedsInto":[4,5,7]},{"GranularBlockName":"UGC","ID":2,"PaperDescription":"Collects user-generated reviews for sentiment analysis.","Inputs":["User comments from travel forums"],"Outputs":["Structured sentiment dataset"],"ReferenceCitation":"Section 3.2.1 We crawl about 1.45 million short comment texts in the unit of POI, including attributes such as POI information, comment content, and evaluation ratings.","FeedsInto":[5,7,15,16]},{"GranularBlockName":"Geographic Information","ID":3,"PaperDescription":"Integrates spatial metadata for route and POI mapping.","Inputs":["Geographic metadata from Amap API"],"Outputs":["Location-based reference system"],"ReferenceCitation":"Section 3.2.1 We collect the geographic information of roughly 2,000 cities (counties) and 30,000 POIs.","FeedsInto":[8,10,11]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Frequent Route Patterns","ID":4,"PaperDescription":"Identifies popular travel route sequences using pattern mining.","Inputs":["Route database","Thematic classifications"],"Outputs":["Frequent route patterns"],"ReferenceCitation":"Section 4.2.1 We use a proposed graph-based substructure pattern mining algorithm called gSpan to mine frequent route patterns.","FeedsInto":[9,10,11,12]}]},{"IntermediateBlockName":"Classification","GranularBlocks":[{"GranularBlockName":"Thematic Route Categorization","ID":5,"PaperDescription":"Categorizes tourism routes into thematic clusters.","Inputs":["UGC keywords","Route metadata"],"Outputs":["Thematic classification of routes"],"ReferenceCitation":"Section 3.2.2 We extract ten themes and their corresponding keywords using the latent Dirichlet allocation (LDA) algorithm.","FeedsInto":[6,12]}]},{"IntermediateBlockName":"Dimension Reduction","GranularBlocks":[{"GranularBlockName":"High-dimensional Route Feature Vectors","ID":6,"PaperDescription":"Reduces high-dimensional route attributes for visualization.","Inputs":["Thematic probability vectors"],"Outputs":["Low-dimensional representation for visualization"],"ReferenceCitation":"Section 4.2.1 The route projection view uses the t-SNE method to reduce the dimensionality of the routes according to the route theme.","FeedsInto":[9]}]},{"IntermediateBlockName":"Indicator Calculation","GranularBlocks":[{"GranularBlockName":"Visit Popularity + Tourist Ratings","ID":7,"PaperDescription":"Computes destination attractiveness based on visit frequency and sentiment analysis.","Inputs":["POI visit data","User reviews","Online ratings"],"Outputs":["Popularity scores","Rating scores"],"ReferenceCitation":"Section 3.2.3 The final tourist rating of each city (POI) is the average of the online rating and comment rating.","FeedsInto":[13]}]},{"IntermediateBlockName":"Optimization","GranularBlocks":[{"GranularBlockName":"Automated Route Optimization","ID":8,"PaperDescription":"Reorders travel plans to minimize travel time.","Inputs":["Route data","POI locations","constraints"],"Outputs":["Optimized travel itinerary"],"ReferenceCitation":"Section 5.1.2 By algorithm, the itinerary was divided into three days automatically, and the daily scheduling duration was limited to a reasonable range.","FeedsInto":[14]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Scatter Plot","ID":9,"PaperDescription":"Displays routes in a scatter plot using t-SNE clustering.","Inputs":["High-dimensional route features"],"Outputs":["2D scatter plot, color-coded by theme"],"ReferenceCitation":"Section 4.2.1 A point represents a route, and each route has ten theme probability values. The color encodes the highest probability theme.","FeedsInto":[17]},{"GranularBlockName":"Donut Chart","ID":12,"PaperDescription":"Displays route theme distributions using a donut chart.","Inputs":["Route clusters","Theme proportions"],"Outputs":["Thematic distribution of routes"],"ReferenceCitation":"Section 4.2.2 The middle donut chart shows which themes of routes the frequent route pattern is mined from.","FeedsInto":[17]},{"GranularBlockName":"Annular Chart","ID":13,"PaperDescription":"Represents route popularity and ratings using annular charts.","Inputs":["Route popularity data","Rating data"],"Outputs":["Comparative visualization of route characteristics"],"ReferenceCitation":"Section 4.2.2 The outermost annular area chart shows the popularity and rating index information.","FeedsInto":[17]},{"GranularBlockName":"Stacked Column Chart","ID":14,"PaperDescription":"Represents trip schedules in a structured format.","Inputs":["User-selected itinerary"],"Outputs":["Hierarchical stacked column visualization of travel plans"],"ReferenceCitation":"Section 4.2.3 We designed a two-layer stacked column chart with a vertical layout, displaying the itinerary's structure clearly.","FeedsInto":[18]},{"GranularBlockName":"Flowchart","ID":15,"PaperDescription":"Displays changes in visitor sentiment over time.","Inputs":["User-generated review sentiments"],"Outputs":["Flowchart showing sentiment distribution trends"],"ReferenceCitation":"Section 4.2.4 The time-series affection view uses a time-series flowchart to express variations in affective states of visitor comments over time.","FeedsInto":[17]},{"GranularBlockName":"Word Cloud","ID":16,"PaperDescription":"Extracts frequent words from user comments, encoding sentiment polarity.","Inputs":["User reviews","Extracted keywords"],"Outputs":["Word cloud encoding sentiment polarity"],"ReferenceCitation":"Section 4.2.4 Keywords are extracted from the UGC comments on the corresponding destination. Its color encodes the corresponding type of sentiment, and size indicates its frequency.","FeedsInto":[17]}]},{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":10,"PaperDescription":"Displays frequent travel routes on a geographic map.","Inputs":["Extracted frequent travel routes"],"Outputs":["Map-based visualization of popular routes"],"ReferenceCitation":"Section 4.2.2 The spatial map occupies the largest visual area, making it convenient for users to observe the geographic distribution and its commercial environment nearby.","FeedsInto":[17,18]},{"GranularBlockName":"Overlay (Route)","ID":11,"PaperDescription":"Overlays route data on a map for deeper insights.","Inputs":["Route metadata"],"Outputs":["Enhanced map visualization with overlays"],"ReferenceCitation":"Section 4.2.2 The frequent route analysis aims to summarize and overview statistical information such as geographic mode, theme distribution, popularity, and ratings.","FeedsInto":[10,17,18]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Route Selection","ID":17,"PaperDescription":"Allows users to filter and select routes based on preferences.","Inputs":["User-defined filters (location, theme, duration)"],"Outputs":["Subset of relevant routes"],"ReferenceCitation":"Section 4.2.1 Users searched for Chengdu and Chongqing as the spatial constraints and selected the default time range as the temporal constraints.","FeedsInto":[4,5,6,7,8,18]}]},{"IntermediateBlockName":"Modification","GranularBlocks":[{"GranularBlockName":"Route Editing","ID":18,"PaperDescription":"Enables users to manually adjust their itineraries.","Inputs":["User interactions (dragging, reordering, adding/removing POIs)"],"Outputs":["Adjusted travel plan"],"ReferenceCitation":"Section 4.2.3 Users also want to adjust the plan intuitively, such as deleting a destination or adjusting the play duration and sequence.","FeedsInto":[8,14]}]}],"HighBlockName":"Interaction"}]},{"Year":2024,"PaperTitle":"HORA 3D: Personalized Flood Risk Visualization as an Interactive Web Service","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"2D Features","ID":1,"PaperDescription":"Loads 2D footprints (e.g., building outlines, perimeter polygons) for flood planning.","Inputs":["Footprint polygons from city data (e.g., shapefiles, OSM)."],"Outputs":["2D feature set for perimeter generation or object protection lines."],"ReferenceCitation":"Sec. 4.1: We define a perimeter as a closed line around important buildings\u2026 uses footprints for geometry.","FeedsInto":[5,11,13,14,16]},{"GranularBlockName":"3D Geometries","ID":2,"PaperDescription":"Loads 3D building models for flood risk visualization.","Inputs":["3D building meshes or extruded shapes."],"Outputs":["Scene geometry for barrier slope checks or advanced 3D visualization."],"ReferenceCitation":"Sec. 4.1: We use detailed 3D building models where available and reconstructed ones otherwise.","FeedsInto":[5,11,16]},{"GranularBlockName":"Digital Elevation Model","ID":3,"PaperDescription":"Loads multi-resolution DEM data to model terrain for flood simulation.","Inputs":["Nested DEMs with resolutions of 2m, 20m, 50m, and 250m."],"Outputs":["Hierarchical terrain representation with adaptive resolution."],"ReferenceCitation":"Sec. 4.1: The terrain consists of four nested digital elevation models (DEM) with increasing cell sizes of 2 m, 20 m, 50 m, and 250 m.","FeedsInto":[5,11]},{"GranularBlockName":"Flood Simulation Data","ID":4,"PaperDescription":"Flood scenarios (water depth, velocity)","Inputs":["Time-series/temporal data: discrete time steps describing water depth & flow velocity fields (continuous)."],"Outputs":["Indexed scenario data for spatiotemporal analysis (flood hazard fields over discrete time steps)."],"ReferenceCitation":"Sec. 3.1: In the HORA 3.0 project, river floods [...] four river flood scenarios have been simulated, from which we derive individual risk factors and visualize them via HORA 3D.","FeedsInto":[6,7,8,9,15]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Rasterization","GranularBlocks":[{"GranularBlockName":"Coverage","ID":5,"PaperDescription":"GPU-based pass or partial rendering for focus+context adaptation.","Inputs":["3D geometry","camera transform","color/alpha scheme","user's current viewpoint."],"Outputs":["Pixel coverage or occlusion metrics used to fade, highlight, or adapt the displayed scene in real time"],"ReferenceCitation":"Sec. 3.2/3.3: We propose an interactive 3D application to inform the general public... A new view-dependent focus+context design guides user attention and supports an intuitive interpretation... We do not show all information at once, but adapt it to the current view.","FeedsInto":[11,12]},{"GranularBlockName":"Water Depth","ID":6,"PaperDescription":"Air/water boundary, building geometry.","Inputs":["Continuous water depth field (time or max depth)","local 3D building geometry."],"Outputs":["Per-building numeric values or per-facade coverage (continuous)."],"ReferenceCitation":"Sec. 3.3: Local water depths around a building often correlate with possible damage. By rotating around the focus building, the user can see the maximum local water depths for each flood scenario, displayed on the facades.","FeedsInto":[11,12]},{"GranularBlockName":"Water Velocity","ID":7,"PaperDescription":"Impact velocity on facades.","Inputs":["Flow velocity data (continuous vectors)","building facade geometry (discrete polygons)."],"Outputs":["Numeric velocity vectors"],"ReferenceCitation":"Sec. 3.3: High water pressure, which correlates with water depth and impact velocity, poses a particular threat to doors. High impact velocities are emphasized by animated arrows moving perpendicular to the facade.","FeedsInto":[12]},{"GranularBlockName":"Water Flow","ID":8,"PaperDescription":"Premises inflow volume.","Inputs":["Flow velocity time-series (continuous)","premises boundary geometry (discrete 2D polygons)."],"Outputs":["Summed/accumulated inflow volumes (numeric) at property edges, for arrow-based representation."],"ReferenceCitation":"Sec. 3.3: The water inflow volume aggregated over the current flood scenario is visualized by arrows placed along the premises border to help identify particularly vulnerable parts.","FeedsInto":[12]},{"GranularBlockName":"Road Accessibility","ID":9,"PaperDescription":"Road accessibility","Inputs":["Continuous water depth data","2D roads geometry (line/polygon)"],"Outputs":["Marked or color-coded roads with discrete states: passable vs. flooded (or continuous measure of depth)."],"ReferenceCitation":"Sec. 3.3: During flooding, the road network can be disrupted [...] We highlight inaccessible roads by coloring them according to their maximum water depth. This is intended to encourage users to early consider possible evacuation routes.","FeedsInto":[12]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":10,"PaperDescription":"2D map for POI selection (Fig. 3 top)","Inputs":["2D geometry (roads, land use)","user viewpoint"],"Outputs":["A top-down orthographic representation (raster) letting users pick or view flooding scenarios in 2D."],"ReferenceCitation":"Sec. 3 (Fig. 3 top + text): The entry point of the application is the HORA website, where users can select a POI by entering an address or picking a point on a 2D map showing areas affected by flooding.","FeedsInto":[13,14]},{"GranularBlockName":"3D Scene","ID":11,"PaperDescription":"3D environment display.","Inputs":["3D geometry (terrain, buildings)","user's camera/view transform","optional 2D overlays (e.g., texture polygons)"],"Outputs":["On-screen 3D environment with interactive visualization."],"ReferenceCitation":"Sec. 3.2: Our application is centered around an interactive 3D visualization to raise awareness. We provide users a selection of visual presets and guide attention via a focus+context design.","FeedsInto":[13,14,15,16]},{"GranularBlockName":"Overlay (Arrow fields, roofs)","ID":12,"PaperDescription":"View-dependent focus+context design.","Inputs":["Risk attributes (continuous data: velocity, depth, inflow)","user's viewpoint & zoom."],"Outputs":["Overlaid arrow fields","colored roofs","partial cutaways that adapt to viewpoint changes"],"ReferenceCitation":"Sec. 3.2-3.3: We provide users with visual presets that adapt based on zoom level, fading details when out of focus.","FeedsInto":[11,10,13,14,15]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Geometry Selection","ID":13,"PaperDescription":"Selecting the premises of interest as focus object.","Inputs":["Map of premises polygons","user's chosen polygon (via click or address search)"],"Outputs":["Focus object and viewpoint updated to the selected premises, triggering local computations (e.g., facade velocities, inflow)."],"ReferenceCitation":"Sect. 3.2: The user selects a premises to explore its flood risk in close-up.","FeedsInto":[5,6,7,8,9]},{"GranularBlockName":"Geometry Search","ID":14,"PaperDescription":"Selecting the Premises of Interest as Focus Object","Inputs":["Map of premises polygons","user's chosen polygon"],"Outputs":["Focus object and viewpoint updated to the selected premises, triggering local computations (e.g., facade velocities, inflow)."],"ReferenceCitation":"Sect. 3.2: The user selects a premises to explore its flood risk in close-up.","FeedsInto":[5,6,7,8,9]},{"GranularBlockName":"Temporal Selection","ID":15,"PaperDescription":"Temporal navigation within a flood scenario.","Inputs":["Time range from the precomputed simulation","user-chosen time step."],"Outputs":["Water depth and flow field for the selected time slice, triggering re-render and re-computations."],"ReferenceCitation":"Sect. 3.3: Users can explore flood propagation over time by browsing time-dependent fields.","FeedsInto":[6,7,8,9]}]},{"IntermediateBlockName":"Manipulation","GranularBlocks":[{"GranularBlockName":"3D Geometry Replacement","ID":16,"PaperDescription":"Protection measures such as walls or sandbags.","Inputs":["User-chosen geometry representing flood barriers (discrete shapes)","premises boundary polygons."],"Outputs":["Updated 3D environment geometry with newly placed barrier shapes, triggering partial re-analysis."],"ReferenceCitation":"Sect. 3.3: Users can test protection measures like sandbags or walls to see required barrier heights and their effect in 3D.","FeedsInto":[5,6,7,8,9]}]}],"HighBlockName":"Interaction"}]},{"Year":2020,"PaperTitle":"Urban Mosaic: Visual Exploration of Streetscapes Using Large-Scale Image Data","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Street-Level Image Dataset","ID":1,"PaperDescription":"We employ a subset of the full data, covering the boroughs of Manhattan and Brooklyn between April 2016 to April 2017, and totaling 7.77 million images.","Inputs":["Raw street-level images from Carmera","Metadata including time, location, and camera orientation"],"Outputs":["Structured dataset of images and metadata"],"ReferenceCitation":"Introducing Urban Mosaic section: We employ a subset of the full data, covering the boroughs of Manhattan and Brooklyn between April 2016 to April 2017, and totaling 7.77 million images.","FeedsInto":[2,3]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Feature Extraction","GranularBlocks":[{"GranularBlockName":"Image Feature Embedding","ID":2,"PaperDescription":"We compute a feature vector of size 4096 for each image region in a 2x2 and a 4x4 grid. Each image is represented by 20 4096-sized feature vectors in total.","Inputs":["Street-level image dataset"],"Outputs":["Feature vectors (512-d for clustering, 4096-d for similarity search)"],"ReferenceCitation":"System Architecture section: We compute a feature vector of size 4096 for each image region in a 2x2 and a 4x4 grid. Each image is represented by 20 4096-sized feature vectors in total.","FeedsInto":[4,5]}]},{"IntermediateBlockName":"Indexing","GranularBlocks":[{"GranularBlockName":"Feature Index","ID":3,"PaperDescription":"We overcome this by trading off accuracy for speed using a locality sensitive hashing (LSH) scheme to encode the feature vectors, and performing the query using the hashed data.","Inputs":["Feature vectors"],"Outputs":["Hashed index of image features"],"ReferenceCitation":"Efficient Image Similarity Computation section: We overcome this by trading off accuracy for speed using a locality sensitive hashing (LSH) scheme to encode the feature vectors, and performing the query using the hashed data.","FeedsInto":[4,5]}]},{"IntermediateBlockName":"Querying","GranularBlocks":[{"GranularBlockName":"Image Similarity Search","ID":4,"PaperDescription":"Two images are said to be similar if the similarity measure between at least one pair of feature vectors from the two images is below a given threshold.","Inputs":["Feature index","Query image"],"Outputs":["Ranked list of similar images"],"ReferenceCitation":"Efficient Image Similarity Computation section: Two images are said to be similar if the similarity measure between at least one pair of feature vectors from the two images is below a given threshold.","FeedsInto":[7,8]},{"GranularBlockName":"Image Clustering","ID":5,"PaperDescription":"This produces a feature vector that coarsely captures general aspects of the scene, to assess overall image similarity and group images into relevant clusters.","Inputs":["Feature index"],"Outputs":["Image clusters"],"ReferenceCitation":"System Architecture section: This produces a feature vector that coarsely captures general aspects of the scene, to assess overall image similarity and group images into relevant clusters.","FeedsInto":[7,8]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Image Mosaic","ID":7,"PaperDescription":"The image mosaic clusters the image query result and visualizes these clusters. The clusters, in this instance, are sorted based on their size.","Inputs":["Image clusters","Similar images"],"Outputs":["Interactive image mosaic view"],"ReferenceCitation":"Urban Mosaic User Interface section: The image mosaic clusters the image query result and visualizes these clusters. The clusters, in this instance, are sorted based on their size.","FeedsInto":[10]}]},{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Map 2D","ID":8,"PaperDescription":"Users can also visualize the spatial distribution of one or more selected clusters on the map, enabling them to more quickly identify locations where images are found with similar attributes.","Inputs":["Spatial metadata from image clusters or query results","Urban data sets (e.g., census, weather)"],"Outputs":["Interactive 2D map view"],"ReferenceCitation":"Urban Mosaic User Interface section: Users can also visualize the spatial distribution of one or more selected clusters on the map, enabling them to more quickly identify locations where images are found with similar attributes.","FeedsInto":[11]},{"GranularBlockName":"Overlay (Heatmap)","ID":9,"PaperDescription":"Urban data sets can be visualized as heatmaps at different spatial resolutions, e.g., neighbourhood, block, street or as a high resolution grid.","Inputs":["Urban data sets"],"Outputs":["Heatmap overlay on 2D map"],"ReferenceCitation":"Urban Data Interface section: Urban data sets can be visualized as heatmaps at different spatial resolutions, e.g., neighbourhood, block, street or as a high resolution grid.","FeedsInto":[8]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Image Query Widget","ID":10,"PaperDescription":"This widget allows users to compose image query constraints. Users can either upload images of interest or use the images already in the underlying database.","Inputs":["Query images"],"Outputs":["Image query constraints"],"ReferenceCitation":"Urban Mosaic User Interface section: This widget allows users to compose image query constraints. Users can either upload images of interest or use the images already in the underlying database.","FeedsInto":[4]},{"GranularBlockName":"Area Selection","ID":11,"PaperDescription":"Based on their exploration of these urban data sets, users can select regions of interest over the map.","Inputs":["User-selected regions"],"Outputs":["Spatial constraints for query and visualization"],"ReferenceCitation":"Urban Data Interface section: Based on their exploration of these urban data sets, users can select regions of interest over the map.","FeedsInto":[4]},{"GranularBlockName":"Temporal Selection","ID":12,"PaperDescription":"Urban data sets can be visualized as heatmaps at different spatial resolutions, e.g., neighbourhood, block, street or as a high resolution grid.","Inputs":["User-selected temporal constraints"],"Outputs":["Temporal constraints for query and visualization"],"ReferenceCitation":"Urban Data Interface section: Urban data sets can be visualized as heatmaps at different spatial resolutions, e.g., neighbourhood, block, street or as a high resolution grid.","FeedsInto":[4]}]}],"HighBlockName":"Interaction"}]},{"Year":2021,"PaperTitle":"CrimAnalyzer: Understanding Crime Sao Paulo","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Crime Data","ID":1,"PaperDescription":"Loads crime records (with timestamps, locations, and crime types) from the provided dataset to create a structured input for analysis.","Inputs":["Crime records (spatial, temporal, and type attributes)"],"Outputs":["An indexed and structured crime dataset ready for analysis"],"ReferenceCitation":"Section 5.2 (Map View): Users visually query the data set by interacting with a map and selecting a region of interest.","FeedsInto":[2,3]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Aggregation","GranularBlocks":[{"GranularBlockName":"Crime Trends","ID":2,"PaperDescription":"Aggregates crime events over time to reveal overall trends such as seasonality and near-repeat victimization patterns.","Inputs":["Time-stamped crime events"],"Outputs":["Time-series statistics that quantify crime occurrence over time"],"ReferenceCitation":"Section 3.3 (Analytical Tasks) & Section 6.3: Dynamic of crimes over time and Seasonality and Near Repeat Victimization are straightforward to be observed with CrimAnalyzer.","FeedsInto":[6,7,8]}]},{"IntermediateBlockName":"Clustering","GranularBlocks":[{"GranularBlockName":"Crime Hotspot Detection","ID":3,"PaperDescription":"Applies Non-Negative Matrix Factorization (NMF) to decompose a matrix (with rows as spatial sites and columns as time slices) in order to identify hotspots based on crime frequency and temporal intensity patterns.","Inputs":["Crime data organized as a sites vs time slices matrix"],"Outputs":["Identified hotspots as clusters (with spatial grouping and corresponding temporal profiles)"],"ReferenceCitation":"Section 4.2 (Hotspot Identification Model): We rely on Non-Negative Matrix Factorization to detect hotspots; the entries in the columns of W highlight sites with high crime prevalence.","FeedsInto":[5,9,10]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":4,"PaperDescription":"Implements a 2D map","Inputs":["2D layer"],"Outputs":["A 2D map"],"ReferenceCitation":"Section 5.2 (Map View): This view is comprised of a geographical map and a choropleth map to encode the number of crimes committed at each site.","FeedsInto":[11,12]},{"GranularBlockName":"Multi-Panel 2D Maps","ID":5,"PaperDescription":"Presents several small 2D maps\u2014one for each detected hotspot\u2014to facilitate spatial comparison among areas with different crime behaviors.","Inputs":["Hotspot clusters (from the NMF-based detection)"],"Outputs":["Multiple 2D maps, each depicting the spatial distribution of a single hotspot"],"ReferenceCitation":"Section 5.3 (Hotspots View): In this view, we use multiple maps to represent the spatial distribution of each hotspot.","FeedsInto":[11,12]},{"GranularBlockName":"Overlay (Choropleth)","ID":9,"PaperDescription":"Implements a 2D choropleth map that color-codes regions based on the density of crimes, providing an immediate spatial overview of crime intensity","Inputs":["Geospatial crime data (site locations and crime counts)"],"Outputs":["A 2D map with a choropleth overlay indicating crime intensity by region"],"ReferenceCitation":"Section 5.2 (Map View): This view is comprised of a geographical map and a choropleth map to encode the number of crimes committed at each site","FeedsInto":[4,5,11,12]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Line Chart","ID":6,"PaperDescription":"Uses a line chart (with an area fill) to illustrate the overall temporal evolution of crime counts, allowing users to perceive trends and fluctuations over continuous time.","Inputs":["Aggregated crime counts over time"],"Outputs":["A line chart showing the evolution of crime intensity across time"],"ReferenceCitation":"Section 5.4 (Global Temporal View): This view gives an overview of the number of crimes committed over the whole time period, relying on a line chart with a filled area between the data value and the base zero line.","FeedsInto":[11,12]},{"GranularBlockName":"Bar Chart","ID":7,"PaperDescription":"Employs a bar chart to display cumulative crime counts over discrete time intervals (e.g., months, days, or periods of the day) to facilitate comparative temporal analysis.","Inputs":["Aggregated crime counts segmented by time intervals"],"Outputs":["A bar chart that depicts the cumulative crime counts per chosen time unit"],"ReferenceCitation":"Section 5.5 (Cumulative Temporal View): This view uses a bar chart to present the number of crimes accumulated by month, day, and period of the day.","FeedsInto":[11,12]},{"GranularBlockName":"Radial Bar Chart","ID":8,"PaperDescription":"Implements multiple bar charts arranged in a radial layout to show seasonal variations in individual crime types over the months; each radial chart represents one crime type across months of each year.","Inputs":["Crime counts per month for each crime type"],"Outputs":["Radial bar charts that expose the seasonal variation of each crime type"],"ReferenceCitation":"Section 5.7 (Radial Type View): In this view, we are using multiple bar charts with a radial layout. Each chart represents a different crime type.","FeedsInto":[11,12]},{"GranularBlockName":"Gauge Chart","ID":10,"PaperDescription":"Displays hotspot metrics via a gauge widget that numerically encodes the number of crimes, the temporal rate of occurrence, and the relative relevance of each hotspot using visual cues (numbers, percentages, pointer).","Inputs":["Hotspot metrics (crime count divided by total crimes, temporal frequency, etc.)"],"Outputs":["A gauge chart for each hotspot, providing an at-a-glance summary of its significance"],"ReferenceCitation":"Section 5.3 (Hotspots View): Below each hotspot there is a gauge widget that depicts the number of crimes, the temporal rate, and the hotspot relevance.","FeedsInto":[11,12]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Policy Selection","ID":11,"PaperDescription":"Allows users to select and filter policies for analysis.","Inputs":["User selection from dropdown."],"Outputs":["Filtered policy adoption visualizations."],"ReferenceCitation":"Section 3.3.1: The drop-down buttons allow the user to select the topic, year ranges, and sorting options.","FeedsInto":[2,3]},{"GranularBlockName":"Mouse Hover","ID":12,"PaperDescription":"Highlights states and policies with contextual information on hover.","Inputs":["User hover action."],"Outputs":["Tooltips with detailed policy adoption data."],"ReferenceCitation":"Section 3.3.2: When the user hovers the mouse over a policy label, a tooltip appears showing contextual factors along with associated hazard ratios.","FeedsInto":[2,3]}]}],"HighBlockName":"Interaction"}]},{"Year":2024,"PaperTitle":"MARLens: Understanding Multi-agent Reinforcement Learning for Traffic Signal Control via Visual Analytics","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Traffic Simulation","ID":1,"PaperDescription":"Simulation Data Loader","Inputs":["Time-series snapshots of an environment (e.g. positions, signals)","environment configuration (2D discrete states, possibly temporal)"],"Outputs":["Organized environment data for each time step (ready for analysis/simulation)."],"ReferenceCitation":"Sec. 6.2: 3) Simulation Data. LibSignal can generate traffic simulation data automatically. [...] The phases of intersections and vehicles' location will be recorded every time step. Simulation data is useful to replay the status of the road network...","FeedsInto":[4,7]},{"GranularBlockName":"Model Weights","ID":2,"PaperDescription":"Model Data Loader","Inputs":["ML model parameters (actor/critic networks)","discrete -> continuous mapping (states to Q-values/actions)"],"Outputs":["Structured model representation describing how states map to decisions (for gleaning or explaining)."],"ReferenceCitation":"Sec. 6.2: 1) Model Data. [...] includes the outputs of both the training and testing models, containing various neural network metrics. We have extracted the input and output data from these networks.","FeedsInto":[5,6]},{"GranularBlockName":"Model Metrics","ID":3,"PaperDescription":"Logger File Parser","Inputs":["Logger files storing (temporal) numeric metrics (reward, queue length, speed loss)"],"Outputs":["Arrays/tables of continuous metrics for each iteration/time step (for distribution & time-series charts)"],"ReferenceCitation":"Sec. 6.2: 2) Logger Files [...] contain various metrics such as mean reward, queue length, delay, and travel time, for each episode. [...] for each episode, we also obtain all the observations [...] current/last action, and current/last phase.","FeedsInto":[9,10]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Training","GranularBlocks":[{"GranularBlockName":"Policy","ID":4,"PaperDescription":"MADDPG Training","Inputs":["Multi-step environment data (temporal states/transitions)","hyperparameters (discrete or continuous)"],"Outputs":["Learned model parameters that transform environment states -> decisions i.e. a trained policy function."],"ReferenceCitation":"Sec. 3.2: We adopt MADDPG as the primary algorithm for our study. [...] We input the road network, traffic flow, [...] fine-tuning the hyperparameters until the selected model effectively controls the traffic lights and attains a satisfactory performance.","FeedsInto":[5,6,11]}]},{"IntermediateBlockName":"Ranking","GranularBlocks":[{"GranularBlockName":"Feature Weights","ID":5,"PaperDescription":"SHAP (Feature Extraction)","Inputs":["ML model I/O logs (continuous/discrete states)","dimensional feature vectors (categorical or numeric)"],"Outputs":["Per-feature importance scores (continuous weights) showing how each feature influences predictions."],"ReferenceCitation":"Sec. 6.3: Similarly, we employ SHAP [42] to analyze the neural networks of each agent and illustrate feature importance. [...] Another model-agnostic approach we employ is SHAP to glean insights into the model training process.","FeedsInto":[8]}]},{"IntermediateBlockName":"Extraction","GranularBlocks":[{"GranularBlockName":"Model Input/Output Sets","ID":6,"PaperDescription":"Decision Tree for Critic/Actor","Inputs":["Model I/O pairs (possibly discrete or continuous states)","environment transitions"],"Outputs":["A hierarchical structure (tree) approximating the ML model's logic (feature checks -> final output)."],"ReferenceCitation":"Sec. 6.3: ...we independently train decision trees for both the critic and actor networks to gain a deeper understanding of how neural networks make decisions. Specifically, we employ all [...] to train a regression tree.","FeedsInto":[15]}]},{"IntermediateBlockName":"Dimension Reduction","GranularBlocks":[{"GranularBlockName":"High-dimensional State Feature Vectors","ID":7,"PaperDescription":"t-SNE Projection of States","Inputs":["High-dimensional vectors (continuous or categorical state attributes, actions, time)"],"Outputs":["2D (or 3D) continuous coordinates for scatter-based arrangement, grouping similar states."],"ReferenceCitation":"Sec. 7.4: ...we utilize the t-SNE algorithm to project the state information, which includes the current decision action, rewards, current time step, [...] into the 2D plane. [...] ensures that similar states are situated closer to each other.","FeedsInto":[12,16]}]},{"IntermediateBlockName":"Mapping","GranularBlocks":[{"GranularBlockName":"Weighted Graph","ID":8,"PaperDescription":"Chord Diagram Generation (relationships)","Inputs":["Cross-entity synergy info (discrete nodes, numeric edge weights from feature influence)"],"Outputs":["Weighted adjacency structure (discrete nodes + continuous edge weights) describing how entities influence each other."],"ReferenceCitation":"Sec. 7.4: We use a chord diagram to display an overview of the influence among agents. [...] A chord from agent A to agent B indicates some features of A affect B's decision-making. The width of the chord encodes the number of features.","FeedsInto":[14]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Distribution Chart","ID":9,"PaperDescription":"Training Distribution","Inputs":["Arrays of numeric metrics (continuous time-series or aggregated stats)","user filters"],"Outputs":["A chart (e.g., boxplot, histogram, lineup) for distribution or ranking of numeric metrics."],"ReferenceCitation":"Sec. 7.2: We devise the Training Distribution, implementing the lineup design [61]. This design choice enables users to rapidly visualize the distribution of the four metrics and swiftly locate their desired episode.","FeedsInto":[17]},{"GranularBlockName":"Line Chart","ID":10,"PaperDescription":"Traffic Signal Phases & Metrics","Inputs":["Time-series data (continuous metric vs. discrete time)","user zoom or selection"],"Outputs":["Pixel-based 2D line chart or multi-line chart"],"ReferenceCitation":"Sec. 7.4: On the right side, [...] we have four sets of lines and bands corresponding to each of the four agents. The line chart effectively illustrates the fluctuations in selected metrics, while bands along the time axis represent traffic signal phases.","FeedsInto":[17,18]},{"GranularBlockName":"Bar Chart","ID":11,"PaperDescription":"(Paper references discrete actions/states)","Inputs":["Discrete categories or labeled data (phase codes, directions, or action indexes)","user selection"],"Outputs":["A bar chart or discrete chart showing category counts, frequencies, or comparisons."],"ReferenceCitation":"Not named explicitly in the text, but implied by references to discrete actions or action probabilities. E.g. in Sec. 7.3, each intersection has four discrete directions. In principle, a bar chart might be used to compare these categorical probabilities.","FeedsInto":[17]},{"GranularBlockName":"Scatter Plot","ID":12,"PaperDescription":"Traffic Condition part of Episode Overview","Inputs":["2D embedding from Compute Embedding","optional color/size encoding (continuous or categorical)"],"Outputs":["On-screen 2D scatter of states/time slices"],"ReferenceCitation":"Sec. 7.3: Traffic Condition. We count the number of vehicles [...] employ the t-SNE algorithm to project traffic conditions onto a 2D plane. Each time step's traffic condition is represented as a point. The color of each point corresponds to its respective time step.","FeedsInto":[17]},{"GranularBlockName":"Chord Diagram","ID":14,"PaperDescription":"Chord diagram in Episode Detail","Inputs":["Weighted adjacency structure from Compute Weighted Influence Graph (discrete nodes, numeric edge weights)"],"Outputs":["Circular chord plot where arcs connect discrete entities, widths or color representing continuous synergy/influence."],"ReferenceCitation":"Sec. 7.4: We use a chord diagram to display an overview of the influence among agents. A chord from agent A to agent B indicates some features of A affect B's decision-making. The width of the chord encodes the number of features.","FeedsInto":[17]},{"GranularBlockName":"Tree Chart","ID":15,"PaperDescription":"Policy Explainer","Inputs":["Hierarchical data from Create Surrogate Model","user selection (which entity/time or condition)"],"Outputs":["Node-link or radial tree showing how features (discrete or continuous) lead to a final decision (leaf node)."],"ReferenceCitation":"Sec. 7.5: We have adopted a tree-based design to present the policies governing the relationship between actions and states. The root node of the tree [...] four subtrees are used to encode rules for four agents, each designated with its respective color.","FeedsInto":[17,19]},{"GranularBlockName":"Glyph","ID":16,"PaperDescription":"Episode Detail: State Projection & Feature Importance","Inputs":["2D embedding","ring-based or radial glyph data (discrete actions, continuous reward, time-step)"],"Outputs":["Overlaid glyph for each point in the scatter, showing multi-attribute data in rings, arcs, or color-coded segments"],"ReferenceCitation":"Sec. 7.4: A circular segment design is adopted to present information about the four agents. We utilize the t-SNE algorithm to project the state info. We also have bar charts for feature importance. Notably, after zooming in, we adopt a glyph design to show more state information, using rings to encode action, traffic flow, reward, and time step.","FeedsInto":[17]}]},{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"2D Map","ID":13,"PaperDescription":"Displays the selected stops name, routes crossing it, route-level stats or arrival-time uncertainties. Also can display arrival time distribution as small charts.","Inputs":["2D layer"],"Outputs":["A table-based info panel for that stops bus routes, uncertainties, or distribution charts."],"ReferenceCitation":"Sec. VI-E: Stop View displays the relevant route IDs, arrival times for the chosen stop","FeedsInto":[17,18]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Area Selection","ID":17,"PaperDescription":"Filter & Select (Lasso & Episode Selection)","Inputs":["Data arrays or embedded sets (discrete, continuous, or time-based)","user bounding box/threshold"],"Outputs":["A subset of data or states is selected, triggering re-filtered visuals."],"ReferenceCitation":"Sec. 7.3: Users can select a group of points of interest with a lasso in Fig. 2C1. We also have sorting episodes by certain metrics in the Training Distribution. The user can select a time step to jump the Simulation Replay.","FeedsInto":[9,10,11,12,13,14,15,16]},{"GranularBlockName":"Temporal Selection","ID":18,"PaperDescription":"Replay Control","Inputs":["Frames from Simulate Environment","user commands (pause/play/jump/adjust time scale)"],"Outputs":["Real-time stepping or continuous playback of environment frames (temporal control)."],"ReferenceCitation":"Sec. 7.6: We have incorporated the Simulation Replay module. [...] allows users to control the simulation, e.g. pausing, adjusting speed, or jumping to a specific time step.","FeedsInto":[10,13]},{"GranularBlockName":"Save Interface State","ID":19,"PaperDescription":"Snapshot Log","Inputs":["Current visual composition (charts, diagrams, etc.)","user command to save"],"Outputs":["Snapshots of the current visual arrangement stored for cross-time or cross-run comparisons."],"ReferenceCitation":"Sec. 7.5: Users can save a snapshot to the Snapshot Log by clicking the 'Save snapshot' button. [...]","FeedsInto":[20]},{"GranularBlockName":"Load Interface State","ID":20,"PaperDescription":"Snapshot Log","Inputs":["Saved visual composition (charts, diagrams, etc.)","user command to load"],"Outputs":["Previously saved snapshots reloaded for comparison or further analysis."],"ReferenceCitation":"Sec. 7.5: [...] They can easily reload a snapshot by clicking it.","FeedsInto":[]}]}],"HighBlockName":"Interaction"}]},{"Year":2019,"PaperTitle":"TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Spatiotemporal datasets","ID":1,"PaperDescription":"Loads large-scale spatio-temporal datasets, such as taxi trip records, store visitor traffic, and regional sales data","Inputs":["Traffic Flow Data (taxi pickup/drop-off locations, timestamps)","Retail Customer Data (store zones, visitor counts, timestamps)","Regional Sales Data (sales regions, product sales, monthly records)"],"Outputs":["Structured dataset formatted as a multi-dimensional tensor for further analysis"],"ReferenceCitation":"Section 3.1: Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by corresponding temporal, spatial and other domain-specific dimensions.","FeedsInto":[2,4]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Partitioning","GranularBlocks":[{"GranularBlockName":"Piecewise Decomposition","ID":2,"PaperDescription":"Performs automatic slicing and partitioning of data for detailed analysis","Inputs":["Multidimensional ST dataset"],"Outputs":["Partitioned subsets of ST data"],"ReferenceCitation":"Section 3.2: We propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns.","FeedsInto":[3,5]}]},{"IntermediateBlockName":"Clustering","GranularBlocks":[{"GranularBlockName":"ST Feature Vectors","ID":3,"PaperDescription":"Uses clustering algorithms to group similar subsets for improved pattern extraction","Inputs":["Feature vectors from tensor decomposition"],"Outputs":["Clusters of similar data patterns"],"ReferenceCitation":"Section 3.2: Given the feature vectors, we can apply a variety of clustering algorithms, including k-means, hierarchical clustering, or OPTICS to cluster the days.","FeedsInto":[4,6,7,8,9]}]},{"IntermediateBlockName":"Approximation","GranularBlocks":[{"GranularBlockName":"Rank-One Approximation","ID":4,"PaperDescription":"Approximates large tensors with rank-one components for easier interpretation","Inputs":["High-dimensional ST tensor"],"Outputs":["Simplified tensor representation with extracted patterns"],"ReferenceCitation":"Section 3.1: We apply successive rank-one CP decomposition methods instead of traditional ones to factorize the original tensor.","FeedsInto":[6,7,8,9,11,10]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Tree View","ID":5,"PaperDescription":"Visualizes hierarchical partitioning of the data","Inputs":["Partitioned subsets of ST data"],"Outputs":["Tree-based visualization"],"ReferenceCitation":"Section 4: The tree starts with a root node... Every other node represents a subset of data. The system supports a steerable and iterative workflow by allowing analysts to interact with every node.","FeedsInto":[17,18]},{"GranularBlockName":"Line Chart","ID":6,"PaperDescription":"Displays extracted patterns along temporal dimensions","Inputs":["Aggregated temporal data"],"Outputs":["Line chart visualization"],"ReferenceCitation":"Section 4.2.1: We visually summarize each partition with a set of simple charts (e.g., line charts) to display the extracted patterns along different dimensions.","FeedsInto":[13,14,15,16]},{"GranularBlockName":"Bar Chart","ID":7,"PaperDescription":"Represents extracted patterns for categorical or numerical distributions","Inputs":["Aggregated categorical/numerical data"],"Outputs":["Bar chart visualization"],"ReferenceCitation":"Section 4.2.1: We visually summarize each partition with a set of simple charts (e.g., bar charts) to display the extracted patterns along different dimensions.","FeedsInto":[13,14,15,16]}]},{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Overlay (Graph)","ID":8,"PaperDescription":"Represents origin-destination movements in ST data","Inputs":["OD data (e.g., taxi trips)"],"Outputs":["Flow map visualization"],"ReferenceCitation":"Section 4.2.1: The flow map shows OD (origin-destination) data which describe spatial movements such as taxi trips with pickup and dropoff locations.","FeedsInto":[12,13,14,15,16]},{"GranularBlockName":"Overlay (Bubble Map)","ID":9,"PaperDescription":"Encodes spatial distribution using size variations","Inputs":["Aggregated spatial data"],"Outputs":["Bubble map visualization"],"ReferenceCitation":"Section 4.2.1: For thematic map, we choose bubble map and use the area of the circles to encode the corresponding values in the 1st loading vectors.","FeedsInto":[12,13,14,15,16]},{"GranularBlockName":"Overlay (Glyph)","ID":10,"PaperDescription":"Encodes multidimensional trends for comparison","Inputs":["Partitioned subsets of ST data"],"Outputs":["Multivariate glyph representations on a map"],"ReferenceCitation":"Section 4.2.2: For the bubble map, we introduce a multivariate glyph design which can be embedded on a map (superposition).","FeedsInto":[12,13,14,15,16]},{"GranularBlockName":"Overlay (Heatmap)","ID":11,"PaperDescription":"Allows superimposing multiple data layers on the same map for comparison.","Inputs":["Multiple spatial datasets"],"Outputs":["Overlaid visual representation on map"],"ReferenceCitation":"Section 4.2.2: Since sharing spatial context facilitates comparative analysis, we overlay multiple data partitions on the same map.","FeedsInto":[12,13,14,15,16]},{"GranularBlockName":"Map 2D","ID":12,"PaperDescription":"Displays spatial data as a 2D base map for overlays and thematic representations.","Inputs":["Base map data (e.g., roads, land use, region boundaries)"],"Outputs":["A rendered 2D map onto which overlays (bubble maps, glyphs, flow maps) are added"],"ReferenceCitation":"Section 4.2.1: For thematic map, we choose bubble map (row #3) and use the area of the circles to encode the corresponding values in the 1st loading vectors.","FeedsInto":[13,14]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Brushing","ID":13,"PaperDescription":"Allows users to brush on plots and maps to select and highlight specific data","Inputs":["User brushing on charts","User brushing on maps"],"Outputs":["Highlighted data subset"],"ReferenceCitation":"Section 2: With brushing and linking, analysts can directly specify queries on the views, select and highlight a subset of data for detailed examination.","FeedsInto":[2,4,14,17]},{"GranularBlockName":"Linking","ID":14,"PaperDescription":"Allows users to interactively link selections across multiple plots and maps","Inputs":["User selection on charts","User selection on maps"],"Outputs":["Linked highlighting across visualizations"],"ReferenceCitation":"Section 2: With brushing and linking, analysts can directly specify queries on the views, select and highlight a subset of data for detailed examination.","FeedsInto":[13,15,16]},{"GranularBlockName":"Superposition","ID":15,"PaperDescription":"Supports overlaying multiple datasets in a single plot or map","Inputs":["Partitioned datasets on charts","Partitioned datasets on maps"],"Outputs":["Overlaid visualizations for comparison"],"ReferenceCitation":"Section 4.2.2: Visual designs for comparison tasks can be categorized into three groups: juxtaposition, superposition, and explicit encoding.","FeedsInto":[11,16]},{"GranularBlockName":"Juxtaposition","ID":16,"PaperDescription":"Supports side-by-side comparison of multiple partitions","Inputs":["Partitioned datasets on charts","Partitioned datasets on maps"],"Outputs":["Juxtaposed visualizations for comparison"],"ReferenceCitation":"Section 4.2.2: Visual designs for comparison tasks can be categorized into three groups: juxtaposition, superposition, and explicit encoding.","FeedsInto":[11,15]}]},{"IntermediateBlockName":"Partition","GranularBlocks":[{"GranularBlockName":"Tree Split","ID":17,"PaperDescription":"Allows users to manually split data partitions in the Tree View","Inputs":["User-defined partition criteria"],"Outputs":["New data partitions"],"ReferenceCitation":"Section 4.2.4: TPFlow supports a steerable and iterative workflow such that analysts can progressively divide the data into smaller subsets along different dimensions.","FeedsInto":[2,5]}]},{"IntermediateBlockName":"Tracking","GranularBlocks":[{"GranularBlockName":"Tree History","ID":18,"PaperDescription":"Enables users to track and refine past partitioning steps in the Tree View","Inputs":["Partitioning history"],"Outputs":["Visualization of past partitioning actions"],"ReferenceCitation":"Section 4.2.4: To support iterative partition refinement and a progressive, top-down analysis workflow, the system should enable analysts to track the steps they have taken to reach the insight.","FeedsInto":[5,17]}]}],"HighBlockName":"Interaction"}]},{"Year":2013,"PaperTitle":"Visual Exploration of Big Spatio-Temporal Urban Data: A Study of New York City Taxi Trips","HighBlocks":[{"IntermediateBlocks":[{"IntermediateBlockName":"Loader","GranularBlocks":[{"GranularBlockName":"Taxi Trip","ID":1,"PaperDescription":"The data used in our study was provided by Taxi and Limousine Commission of New York City and contains information about all medallion taxi trips in 2009, 2011, and 2012.","Inputs":["Raw CSV files from NYC Taxi and Limousine Commission"],"Outputs":["Structured dataset of taxi trips with attributes like trip id, taxi id, pickup/dropoff locations and times, distance, fare amount, tip amount, and toll amount"],"ReferenceCitation":"Data and Design Requirements section: The data used in our study was provided by Taxi and Limousine Commission of New York City and contains information about all medallion taxi trips in 2009, 2011, and 2012.","FeedsInto":[2]}]}],"HighBlockName":"Data Loading"},{"IntermediateBlocks":[{"IntermediateBlockName":"Indexing","GranularBlocks":[{"GranularBlockName":"Spatiotemporal Index","ID":2,"PaperDescription":"We have built a specialized index based on k-d trees to support spatio-temporal queries at interactive rates.","Inputs":["Structured dataset of taxi trips"],"Outputs":["Indexed data structure supporting spatio-temporal constraints"],"ReferenceCitation":"The TaxiVis System section: We have built a specialized index based on k-d trees to support spatio-temporal queries at interactive rates.","FeedsInto":[3]}]},{"IntermediateBlockName":"Querying","GranularBlocks":[{"GranularBlockName":"Visual Query Engine","ID":3,"PaperDescription":"Users formulate queries visually, by interacting with maps and other visual representations. Internally, a textual query is generated which is then evaluated by the storage manager.","Inputs":["User-defined visual queries","Indexed data structure"],"Outputs":["Query result set (subset of taxi trips)"],"ReferenceCitation":"The TaxiVis System section: Users formulate queries visually, by interacting with maps and other visual representations. Internally, a textual query is generated which is then evaluated by the storage manager.","FeedsInto":[6,7,8,9,10,11]}]}],"HighBlockName":"Data Processing"},{"IntermediateBlocks":[{"IntermediateBlockName":"Geospatial","GranularBlocks":[{"GranularBlockName":"Map 2D","ID":6,"PaperDescription":"Maps serve different purposes in our system. They provide a canvas for displaying query results, for users to specify spatial constraints and compose/refine queries.","Inputs":["Query result set"],"Outputs":["Interactive 2D map view with pickups, drop-offs, directional constraints"],"ReferenceCitation":"User Interface Components section: Maps serve different purposes in our system. They provide a canvas for displaying query results, for users to specify spatial constraints and compose/refine queries.","FeedsInto":[12]},{"GranularBlockName":"Overlay (Heatmap)","ID":7,"PaperDescription":"Our system supports density summary visualizations or heat maps that can be used to show the distribution of pickups and/or dropoffs in an area.","Inputs":["Query result set"],"Outputs":["Heatmap overlay on 2D map"],"ReferenceCitation":"Query Result Visualization section: Our system supports density summary visualizations or heat maps that can be used to show the distribution of pickups and/or dropoffs in an area.","FeedsInto":[6,12]},{"GranularBlockName":"Grid Map","ID":8,"PaperDescription":"A grid map is a set of cells where their geometries and visual representations can be customized by the users.","Inputs":["Query result set"],"Outputs":["Grid-based map visualization by region (e.g., neighborhoods, zip codes)"],"ReferenceCitation":"Query Result Visualization section: A grid map is a set of cells where their geometries and visual representations can be customized by the users.","FeedsInto":[12]}]},{"IntermediateBlockName":"Infovis","GranularBlocks":[{"GranularBlockName":"Time Series","ID":9,"PaperDescription":"The information associated with the results of a query can be visualized using different representations within the data summary view, such as time series.","Inputs":["Query result set"],"Outputs":["Time series plots of taxi trips over time"],"ReferenceCitation":"User Interface Components section: The information associated with the results of a query can be visualized using different representations within the data summary view, such as time series.","FeedsInto":[13]},{"GranularBlockName":"Histogram","ID":10,"PaperDescription":"The attribute selection view shows histograms that summarize the attribute values for the trips in the result set of the query.","Inputs":["Query result set"],"Outputs":["Histograms of attribute distributions (e.g., fare, tip amount)"],"ReferenceCitation":"Visual Query Specification section: The attribute selection view shows histograms that summarize the attribute values for the trips in the result set of the query.","FeedsInto":[14]},{"GranularBlockName":"Scatter Plot","ID":11,"PaperDescription":"Scatter plots are used to examine the duration of trips to the airports at different times of the day.","Inputs":["Query result set"],"Outputs":["Scatter plots showing relationships between attributes (e.g., trip duration vs. time of day)"],"ReferenceCitation":"Exploring Query Results section: Scatter plots are used to examine the duration of trips to the airports at different times of the day.","FeedsInto":[14]}]}],"HighBlockName":"Visualization"},{"IntermediateBlocks":[{"IntermediateBlockName":"Filter","GranularBlocks":[{"GranularBlockName":"Area Selection","ID":12,"PaperDescription":"Spatial constraints are specified by polygons and arrows on the map view. These are created either by brushing or by selecting predefined polygons corresponding to NYC's neighborhoods, zip codes, and boroughs.","Inputs":["User selection on map (polygons, arrows)"],"Outputs":["Spatial query constraints"],"ReferenceCitation":"Visual Query Specification section: Spatial constraints are specified by polygons and arrows on the map view. These are created either by brushing or by selecting predefined polygons corresponding to NYC's neighborhoods, zip codes, and boroughs.","FeedsInto":[3]},{"GranularBlockName":"Temporal Selection","ID":13,"PaperDescription":"Temporal constraints are specified using the time selection widgets. In the Regular Selection widget, the user defines an atomic temporal constraint by assigning the values of the start time and end time fields.","Inputs":["User selection on time widget"],"Outputs":["Temporal query constraints"],"ReferenceCitation":"Visual Query Specification section: Temporal constraints are specified using the time selection widgets. In the Regular Selection widget, the user defines an atomic temporal constraint by assigning the values of the start time and end time fields.","FeedsInto":[3]},{"GranularBlockName":"Attribute Filter","ID":14,"PaperDescription":"Attribute constraints are defined through the attribute selection view, which shows histograms that summarize the attribute values for the trips in the result set of the query.","Inputs":["User selection on attribute histograms"],"Outputs":["Attribute query constraints"],"ReferenceCitation":"Visual Query Specification section: Attribute constraints are defined through the attribute selection view, which shows histograms that summarize the attribute values for the trips in the result set of the query.","FeedsInto":[3]}]}],"HighBlockName":"Interaction"}]}]}